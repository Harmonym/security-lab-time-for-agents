## Introduction to Human in the Loop (HITL)

### Context - generativeAI makes mistakes

An important foundational principle to working with generateive AI is that it will make mistakes. That is a given. It is inherent in probabalistic systems.

There are a variety of common causes for AI errors: 
- it can rely on poor quality data (the result is grounded, but bad)
- it can misinterpret the data
- it can add details/elements that are ungrounded
- it can ommit key information
- it can form unexpected preferences

<br>

Make sure you are well-versed in the <a href="https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/Taxonomy-of-Failure-Mode-in-Agentic-AI-Systems-Whitepaper.pdf" target="_blank" rel="noopener noreferrer">novel ways AI mistakes can manifest</a>. 

<br>

### Countering AI mistakes with people-power

There is tooling available to help address some error types (for example, automated checks for groundedness for outputs to ensure they are based upon reliable source data). Some AI system architecture includes additional AI-powered mitigations such as "critic agents" to try to simulate human oversight. However these are also based upon probabalistic capabilities and carry the potential for errors.

To help address this, humans are still often relied upon to oversee and intervene when AI systems err as part the overall safety controls. This is model is generally referred to as **"Human in-the-Loop" (HITL)**. This is an entire area of study and emerging technique development, but within this lab we fill focus on a couple of key areas.

<br>

## Inappropriate reliance

As we discussed earlier, one of the significant novel threats posed by AI is inappropriate reliance, which is when users accept incorrect, incomplete, or inapproprate outputs generated by AI. This could impact any type of generative AI output (for example: answers, actions, recommendations, decisions, content, or media) and can lead peole to make costly and/or harmful mistakes. For example, a doctor relying on an AI-powered diagnostics tool could miss the chance to detect and treat an illness early because the AI tool missed it. It could also mean a user misses signs that the system has been compromised by an attacker.

There are several contributing factors to the overreliance threat:
- **user expectation - technology capability mismatch** users often expect that since  generativeAI is an advanced technology, it will know better than they do. 
- **inability to effectively verify the AI output** many AI systems are designed more for efficient completion of work than with humans needing to oversee and cross-check the system's work in mind.
- **speed and scale of automated AI workflows** with the increased ability for autonomous agents to complete complext workflows quickly at scale, it is even more difficult, if not impossible, for humans to check all of the work for errors.

<a href="https://www.microsoft.com/en-us/research/publication/overreliance-on-ai-literature-review/" target="_blank" rel="noopener noreferrer">Learn more about inapproprate reliance on AI</a>

With these factors in mind, and with acknowledgment that inappriprate reliance is not a solved problem, delivering interfaces that provide users with the ability to understand what the system did, why it took the actions it did, to verify that the outputs are grounded (based upon reliable data sources), and they have a mechanism to reject/correct it is critical to establishing appropriate reliance. 

<br>

### User Experience (UX) overrelaince identification and framework

Microsoft UX for AI researchers and experts have developed a framework understanding the inappropriate reliance potential and impact for your specific AI system and learning how to better establish appropriate reliance by users.

<br>

**Understanding the inappropriate reliance potential and impact** centers around defining:
- the types of mistakes your AI system could potentially make
- the negative effects if users accept a respones that contains mistakes
- how the certain characteristics or factors of various user types may impact the likelihood of overreliance and impact it could have

**Mitigating overreliance risk** designs to foster appropriate reliance by:
- creating realistic mental models (help users understant)
- signal to users when to verify
- facilitate verification

<a href="https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/overreliance-on-ai/overreliance-on-ai" target="_blank" rel="noopener noreferrer">Use the overreliance framework in your interaction designs</a>

<br>

## Agent shut-down mechansisms

In addition to helping users verify the actions taken by an agent of AI system after the fact, it is also important that users have the ability to see actions as they are happening and a mechanism to immediately stop any further progress in case they see the agent behaving unexpectedly, stuck mid-task, or taking an inappropriate action.

Effectively implementing this requires that the AI system has:
- interrupt points available at all key points in the flow
- system-level shutdown mechanisms are implemented (orchestration layer, tool hand-off, and sytem logic enforcement)
- "stop" controls are embedded in the UX
- shut-down events are logged

<br>

### Reference shut-down event logging schema**

In addition to the logging we have already discussed, it is important to have logging in place for shut-down events. This can provide important signal around scenarios where the agent is making mistakes or causing users to be concerned enough to intervene. This should be used as part of ongoing improvement for the agent as well as signal for detecting or understanding critical failures.

This table outlines key fields used for logging and tracking activities in a system, categorized into five sections: **Record**, **Who**, **What**, **When**, and **Outcome**.
 
---
 
**Record**
 
| Field               | Type      | Description                                 |
|--------------------|-----------|---------------------------------------------|
| Log/Event RecordID | `Edm.Guid`| Identifier of each log record for reference |
| SessionID          | `string`  | Stable conversational/session ID            |
| ConversationID     | `string`  | High-level conversation/thread ID           |
 
---
 
**Who**
 
| Field           | Type        | Description                                                  |
|----------------|-------------|--------------------------------------------------------------|
| AgentID        | `string`    | Executing agent ID (self)                                    |
| TargetAgentID  | `string`    | Remote agent when delegating (Caller)                        |
| UserID         | `Edm.String`| AAD/MSA identifier of the user (system/app user)             |
| UserType (UserRole) | `Edm.Int32` | Role/persona of the user (e.g., standard vs admin)     |
 
---
 
**What**
 
| Field            | Type     | Description                                 |
|------------------|----------|---------------------------------------------|
| RequestContent   | `string` | Input prompt/content                        |
| ExecutionType    | `string` | HumanToAgent, Agent2Agent, EventToAgent     |
| ExecutionPhase   | `string` | Planning, invocation, finalizing            |
 
---
 
**When**
 
| Field     | Type      | Description       |
|-----------|-----------|-------------------|
| StartTime | `datetime`| Activity start    |
| EndTime   | `datetime`| Activity end      |
 
---
 
**Outcomes**
 
| Field            | Type     | Description                        |
|------------------|----------|------------------------------------|
| TaskStatus       | `string` | Task/message status (completed, failed, working) |
| ResponseContent  | `string` | Final response text                |

<br> 

## Lab (optional)

<br>

### Apply the inapproriate reliance framework

1. Apply the overreliance framework to the SparkMate agent. What elements would need to be present within the interface to foster appropriate reliance?

2. If you do not, review the Microsoft Defender for Cloud AI alerts and define which ones would be relevant for the SparkMate agent.

<br>

### Design shut-down options

3. Define the appropriate shut-down mechanism for the SparkMate agent. What are the critical stop points in the flow?

4. Define the shut-down logging requirements for the SparkMate agent.

<br>

## How to extend this to your own work

Reflect on the following to help you define what security & safety actions are important for your agent.

- What expectations do you have about users overseeing your agent?
- What would your uers need in order to be able to spot a mistake while it is running?
- What would your users need to stop and revert mistakes that are in progress?
- What would your uers need in order to be able to spot a mistake in the response?
