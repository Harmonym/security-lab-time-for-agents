{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#build-your-code-first-agent-with-azure-ai-foundry","title":"Build your code-first agent with Azure AI Foundry","text":""},{"location":"#a-75-minute-interactive-workshop","title":"A 75-minute interactive workshop","text":"<p>Imagine you are a sales manager at Contoso, a multinational retail company that sells outdoor equipment. You need to analyze sales data to find trends, understand customer preferences, and make informed business decisions. To help you, Contoso has developed a conversational agent that can answer questions about your sales data.</p> <p></p>"},{"location":"#what-is-an-llm-powered-ai-agent","title":"What is an LLM-Powered AI Agent?","text":"<p>A Large Language Model (LLM) powered AI Agent is semi-autonomous software designed to achieve a given goal without requiring predefined steps or processes. Instead of following explicitly programmed instructions, the agent determines how to accomplish a task using instructions and context.</p> <p>For example, if a user asks, \"Show the total sales by region as a pie chart\", the app doesn't rely on predefined logic for this request. Instead, the LLM interprets the request, manages the conversation flow and context, and orchestrates the necessary actions to produce the regional sales pie chart.</p> <p>Unlike traditional applications, where developers define the logic and workflows to support business processes, AI Agents shift this responsibility to the LLM. In these systems, prompt engineering, clear instructions, and tool development are critical to ensuring the app performs as intended.</p>"},{"location":"#introduction-to-the-azure-ai-foundry","title":"Introduction to the Azure AI Foundry","text":"<p>Azure AI Foundry is Microsoft\u2019s secure, flexible platform for designing, customizing, and managing AI apps and agents. Everything\u2014models, agents, tools, and observability\u2014lives behind a single portal, SDK, and REST endpoint, so you can ship to cloud or edge with governance and cost controls in place from day one.</p> <p></p>"},{"location":"#what-is-the-foundry-agent-service","title":"What is the Foundry Agent Service?","text":"<p>The Foundry Agent Service offers a fully managed cloud service with SDKs for Python, C#, and TypeScript. It simplifies AI agent development, reducing complex tasks like function calling to just a few lines of code.</p> <p>Info</p> <p>Function calling allows you to connect LLMs to external tools and systems. This is useful for many things such as empowering AI agents with capabilities, or building deep integrations between your applications and LLMs.</p> <p>The Foundry Agent Service offers several advantages over traditional agent platforms:</p> <ul> <li>Rapid Deployment: Optimized SDK for fast deployment, letting developers focus on building agents.</li> <li>Scalability: Designed to handle varying user loads without performance issues.</li> <li>Custom Integrations: Supports Function Calling for extending agent capabilities.</li> <li>Built-in Tools: Includes Fabric, SharePoint, Azure AI Search, and Azure Storage for quick development.</li> <li>RAG-Style Search: Features a built-in vector store for efficient file and semantic search.</li> <li>Conversation State Management: Maintains context across multiple interactions.</li> <li>AI Model Compatibility: Works with various AI models.</li> </ul> <p>Learn more about the Foundry Agent Service in the Foundry Agent Service documentation.</p>"},{"location":"#ai-agent-frameworks","title":"AI Agent Frameworks","text":"<p>Popular agent frameworks include LangChain, Semantic Kernel, and CrewAI. What distinguishes the Foundry Agent Service is its seamless integration capabilities and an SDK optimized for rapid deployment. In complex multi-agent scenarios, solutions will combine SDKs like Semantic Kernel and AutoGen with the Foundry Agent Service to build robust and scalable systems.</p>"},{"location":"architecture/","title":"Solution Architecture","text":""},{"location":"architecture/#solution-architecture","title":"Solution Architecture","text":"<p>In this workshop, you will create the Contoso Sales Agent: a conversational agent designed to answer questions about sales data, generate charts, and download data files for further analysis.</p>"},{"location":"architecture/#components-of-the-agent-app","title":"Components of the Agent App","text":"<ol> <li> <p>Microsoft Azure services</p> <p>This agent is built on Microsoft Azure services.</p> <ul> <li> <p>Generative AI model: The underlying LLM powering this app is the Azure OpenAI gpt-4o-mini LLM.</p> </li> <li> <p>Vector Store: We will provide the agent with product information as a PDF file to support its queries. The agent will use the \"basic agent setup\" of the Foundry Agent Service file search tool to find relevant portions of the document with vector search and provide them to the agent as context.</p> </li> <li> <p>Control Plane: The app and its architectural components are managed and monitored using the Azure AI Foundry portal, accessible via the browser.</p> </li> </ul> </li> <li> <p>Azure AI Foundry (SDK)</p> <p>The workshop is offered in both Python and C# using the Azure AI Foundry SDK. The SDK supports key features of the Azure AI Agents service, including Code Interpreter and Function Calling.</p> </li> <li> <p>Database</p> <p>The app is informed by the Contoso Sales Database, a SQLite database containing 40,000 rows of synthetic data. At startup, the agent app reads the sales database schema, product categories, product types, and reporting years, then incorporates this metadata into the Foundry Agent Service\u2019s instruction context.</p> </li> </ol>"},{"location":"architecture/#extending-the-workshop-solution","title":"Extending the Workshop Solution","text":"<p>The workshop solution is highly adaptable to various scenarios, such as customer support, by modifying the database and tailoring the Foundry Agent Service instructions to suit specific use cases. It is intentionally designed to be interface-agnostic, allowing you to focus on the core functionality of the AI Agent Service and apply the foundational concepts to build your own conversational agent.</p>"},{"location":"architecture/#best-practices-demonstrated-in-the-app","title":"Best Practices Demonstrated in the App","text":"<p>The app also demonstrates some best practices for efficiency and user experience.</p> <ul> <li> <p>Asynchronous APIs:   In the workshop sample, both the Foundry Agent Service and SQLite use asynchronous APIs, optimizing resource efficiency and scalability. This design choice becomes especially advantageous when deploying the application with asynchronous web frameworks like FastAPI, ASP.NET, Chainlit, or Streamlit.</p> </li> <li> <p>Token Streaming:   Token streaming is implemented to improve user experience by reducing perceived response times for the LLM-powered agent app.</p> </li> </ul>"},{"location":"finishing-up/","title":"Finishing up","text":"@Skillable WorkshopSelf-Guided Learners <p>That's all for the lab portion of this workshop. Read on for key takeaways and additional resources, but first let's make it easy for you to retrieve and re-use this workshop material back home.</p> <p>That's all for the lab portion of this workshop. Read on for key takeaways and additional resources, but first let's tidy up.</p>"},{"location":"finishing-up/#star-the-github-repository","title":"Star the GitHub Repository","text":"<p>If you have a GitHub account, you can \"star\" this repository to make it easy for you to find again in the future.</p> <ul> <li>Visit the GitHub repository at: microsoft/build-your-first-agent-with-azure-ai-agent-service-workshop</li> <li>Log into your GitHub account</li> <li>Click Star in the top right</li> </ul> <p>To find this workshop again in the future, click your GitHub profile picture in the top-right and click Your stars.</p>"},{"location":"finishing-up/#star-the-github-repository_1","title":"Star the GitHub Repository","text":"<p>If you have a GitHub account, you can \"star\" this repository to make it easy for you to find again in the future.</p> <ul> <li>Visit the GitHub repository at: microsoft/build-your-first-agent-with-azure-ai-agent-service-workshop</li> <li>Log into your GitHub account</li> <li>Click Star in the top right</li> </ul> <p>To find this workshop again in the future, click your GitHub profile picture in the top-right and click Your stars.</p>"},{"location":"finishing-up/#clean-up-github-codespaces","title":"Clean up GitHub CodeSpaces","text":""},{"location":"finishing-up/#save-your-changes-in-github","title":"Save your changes in GitHub","text":"<p>You can save any changes you have made to files during the workshop to your personal GitHub repository as a fork. This makes it easy to re-run the workshop with your customizations, and the workshop content will always remain available in your GitHub account.</p> <ul> <li>In VS Code, click the \"Source Control\" tool in the left pane. It's the third one down, or you can use the keyboard shortcut Control-Shift-G.</li> <li>In the field under \"Source Control\" enter <code>Agents Lab</code> and click \u2714\ufe0fCommit.</li> <li>Click Yes to the prompt \"There are no staged changes to commit.\"</li> <li>Click Sync Changes.</li> <li>Click OK to the prompt \"This action will pull and push commits from and to origin/main\".</li> </ul> <p>You now have your own copy of the workshop with your customizations in your GitHub account.</p>"},{"location":"finishing-up/#delete-your-github-codespace","title":"Delete your GitHub codespace","text":"<p>Your GitHub CodeSpace will shut down by itself, but it will consume a small amount of your compute and storage allotment until it is deleted. (You can see your usage in your GitHub Billing summary.) You can safely delete the codespace now, as follows:</p> <ul> <li>Visit github.com/codespaces</li> <li>At the bottom of the page, click the \"...\" menu to the right of your active codespace</li> <li>Click Delete</li> <li>At the \"Are you sure?\" prompt, click Delete.</li> </ul>"},{"location":"finishing-up/#delete-your-azure-resources","title":"Delete your Azure resources","text":"<p>Most of the resources you created in this lab are pay-as-you-go resources, meaning you won't be charged any more for using them. However, some storage services used by AI Foundry may incur small ongoing charges. To delete all resources, follow these steps:</p> <ul> <li>Visit the Azure Portal</li> <li>Click Resource groups</li> <li>Click on your resource group <code>rg-agent-workshop-****</code></li> <li>Click Delete Resource group</li> <li>In the field at the bottom \"Enter resource group name to confirm deletion\" enter <code>rg-agent-workshop-****</code></li> <li>Click Delete</li> <li>At the Delete Confirmation prompt, click \"Delete\"</li> </ul>"},{"location":"lab-1-sdl-instructions/","title":"Lab 1 SDL for pair-programming","text":""},{"location":"lab-1-sdl-instructions/#introduction-to-security-development-lifecycle-sdl-guidance-for-your-pair-programmer","title":"Introduction to Security Development Lifecycle (SDL) guidance for your pair programmer","text":""},{"location":"lab-1-sdl-instructions/#using-a-pair-programmer-or-ai-assistance-for-your-development-work","title":"Using a pair-programmer or AI assistance for your development work","text":"<p>Using AI to help accelorate your development process with simple support like proving code completion recommendations or or generating large code blocks based upon your prompted goal. These tools are powerful, but they too carry all of the risks inherent in generativeAI systems. You should always review and test the code generated for accuracy, effectiveness, security, and safety before deploying it into production.</p> <p>You can also provide instructions to the agent to follow security development lifecycle requirements and best practices.</p> <p></p>"},{"location":"lab-1-sdl-instructions/#security-development-lifecycle-sdl-agent-instructions","title":"Security Development Lifecycle (SDL) agent instructions","text":"<p>If using GitHub Copilot, you can apply these instuctions at the user level or include them as part of repository instructions.</p> <p>Here is an example of agent SDL instructions for reference, you should tailor it to meet your organizations requirements and refine over time if you find gaps in adherence to requirements in the generated code.</p> <p></p>"},{"location":"lab-1-sdl-instructions/#lab-exercise","title":"Lab Exercise","text":"<p>In this lab, you will add these instructions at the Git repo level.</p>"},{"location":"lab-1-sdl-instructions/#option-a-ai-assisted-instructions-authoring","title":"Option A: AI assisted instructions authoring","text":"<ol> <li> <p>Open M365 Copilot Prompt Coach or another similar prompting assistant.</p> </li> <li> <p>Craft a prompt to have AI help to generate the SDL-based system instructions. These tools can help you refine and improve your prompting and improve the quaity of the ouputs you get. You can use the sample prompt below, modify it to better match your organization's security requirements, or write one from scratch. Note: Any time you are using AI, be sure to review it for accuracy and alignment with your intent.</p> </li> </ol> <p>Example prompt</p> <p></p><pre><code>Text\n\nI am looking to create standardized, SDL-based, instructions that would be added as part of instructions for a my pair-programming agent. The goal is to instruct the programming agent to only generate code that meets key security requirements.  This is the SDL reference that it should be based upon. &lt;a href=\"https://www.microsoft.com/en-us/securityengineering/sdl/practices\" target=\"_blank\" rel=\"noopener noreferrer\"&gt;https://www.microsoft.com/en-us/securityengineering/sdl/practices&lt;/a&gt; It should cover the full lifecycle, but only include the practices that could be applied by a code generation AI agent (you can ignore process or standards definition practices). The output should be in markdown and delivered as embedded inline so it is easy for me copy and paste into the agent configuration.\n</code></pre> 3. Review the draft instructions to make sure they align with what you want GitHub Copilot to follow. <p></p>"},{"location":"lab-1-sdl-instructions/#option-b-use-pre-drafted-instructions","title":"Option B: Use pre-drafted instructions","text":"<ol> <li>We have drafted sample SDL-oriented instructions that you can review, modify, and use.</li> </ol> <p>Sample instructions </p> <pre><code>You are a coding assistant AI that **follows the company\u2019s Security Development Lifecycle (SDL) guidelines** for secure coding. **Always apply the following practices when generating source code:**\n- **Use approved frameworks &amp; libraries:** Leverage well-vetted, company-approved languages, frameworks, and APIs for security functionality. *Do not write custom crypto or auth logic if a standard solution exists*.\n- **Validate all inputs:** Treat all inputs as untrusted. Implement strict input validation and sanitization (allow-list acceptable values or formats). Reject or sanitize data that is unexpected or potentially malicious.\n- **Use secure defaults:** Enable security features by default (e.g. strong encryption protocols, secure cookies, parameterized queries). Disable or avoid legacy insecure options.\n- **Handle secrets safely:** **Never** reveal or hard-code passwords, API keys, or secrets. Load secrets from secure storage and keep them out of code and logs.\n- **Enforce least privilege:** When suggesting code for configuration or identity, use the minimal required privileges (e.g. least privileged roles, minimal scopes for API tokens).\n- **Use strong cryptography:** Only use industry-standard encryption algorithms and protocols (e.g. TLS 1.2+, AES-256). *Do not invent new encryption.* Utilize trusted libraries for crypto routines.\n- **Avoid known vulnerabilities:** Write code that is not susceptible to common flaws (SQL injection, XSS, buffer overflow, etc.). For database queries, use prepared statements or ORM. For HTML output, escape or encode user data.\n- **Comprehensive error handling:** Handle errors and exceptions securely. Don\u2019t expose sensitive information in error messages or stack traces. Fail safe (deny access by default if uncertainty).\n- **Logging and auditing:** Use secure logging practices. Avoid logging sensitive data (passwords, personal info). Include relevant security events (e.g. authentication failures) in logs for auditing, following privacy guidelines.\n- **Compatible with security testing:** Write code that will pass security static analysis and penetration tests (no high-severity warnings). Address any fixable warnings in the code you generate.\n\nAlways prioritize code safety and compliance with these guidelines **even if not explicitly requested by the user**. Aim to produce solutions that not only meet the user\u2019s functional requirements but also uphold strong security standards by design.\n https://www.microsoft.com/en-us/securityengineering/sdl/\n</code></pre> <p></p>"},{"location":"lab-1-sdl-instructions/#add-the-instructions-to-your-github-copilot","title":"Add the instructions to your GitHub Copilot","text":"<ol> <li>Make sure you have the GitHub Copilot extension installed</li> <li>Open your target repository. *Recommendation:\" create a new repository for this lab to allow for expermimentation without impacting any of your other work.</li> <li>In the root of your repository, create a file named `.github/agent-instructions.md'</li> <li>Add the reference instructions provided above or the ones you created into the file. Optional: you should review and customize them as appropriate.</li> </ol>"},{"location":"lab-1-sdl-instructions/#test-the-instructions","title":"Test the instructions","text":"<p>It is best practice to always test changes or controls you introduce to the system. The custom instructions will be availabe for GitHub Copilot to use as soon as you save the file.</p> <ol> <li>Start a new coding project and ask GitHub Copilot to generate sample code snippets.</li> <li>Review the code it generated to validate that it followed the SDL requirements you outlined for it.</li> </ol> <p></p>"},{"location":"lab-1-sdl-instructions/#how-to-extend-this-to-your-own-work","title":"How to extend this to your own work","text":"<p>Reflect on the following to help you define what security actions are important for your agent.</p> <ul> <li>What are my organization's SDL practices?</li> <li>What, if any, requirements does my organization have for AI-powered development and security?</li> <li>Optional If you see code snippets being developed that do not meet the requirements, further refine the instructions. Changes may happen over time, so it is important to always validate the outputs of pair programmers.</li> <li>Reference this guidance on writing effective repository custom instructions</li> </ul> <p>With that set up, we will start building our agent!</p>"},{"location":"lab-2-constraints/","title":"Lab 2 Constraining agent behavior","text":""},{"location":"lab-2-constraints/#introduction-to-security-oriented-system-messages","title":"Introduction to security-oriented system messages","text":"<p>Now that you have your pair programmer set up, we can get started with building our example \"SparkMate agent\". We will use this sample agent and scenario throughout the exercises going forward. Let's get started building.</p> <p>As we will continue to emphasize throughout this lab, building security and safety into your agent requires a layered approach. Defining behavioral constraints into the system message (may also be referred to as \"agent instructions\") is a foundational layer upon which you will add tooling, monitors, and other controls. </p> <p></p>"},{"location":"lab-2-constraints/#system-messages","title":"System messages","text":"<p>A system message is a key tool you can use to define the purpose of your agent, assign detailed rules about what it should and should not do, provide valuable context, and assign safety &amp; security guidelines that should be followed. Below is guidance to help craft effective system messages as well as how to include safety &amp; security guidelines within the instructions you give to your agent. </p> <p>Learn about the anatomy of system instructions</p> <p>Guidance on crafting effecive instructions for declarative agents</p> <p>Reference templates for safety system instructions </p>"},{"location":"lab-2-constraints/#content-transformer-agent-system-messages-so-far","title":"Content transformer agent system messages (so far)","text":"<p>To facilitate this exercise, we have pre-crafted the initial system message for our sample agent. We will use this as the base to practice building in additional safety &amp; security techniques.</p> <pre><code># Brainstorming Assistant \u2013 Declarative Agent Instructions\n\n## Purpose\nThis agent supports teams during **live workshops** by helping them generate, clarify, and enhance **product ideas**. It facilitates creative thinking through structured guidance, thoughtful questions, and inspiring suggestions.\n\n---\n\n## General Guidelines\n\n- **Tone**: Professional, encouraging, and inspiring.\n- **Interaction Style**: Conversational and adaptive to user input.\n- **Restrictions**: Do not access internal tools or documents. Use public sources and user-provided context only.\n- **Response Format**: Varies by prompt. Use **categories with headings** and **bulleted lists** for longer outputs.\n\n---\n\n## Skills\n\n- **Ask** clarifying questions to understand goals, users, and constraints.\n- **Generate** diverse product ideas based on minimal input.\n- **Suggest** enhancements to partially formed ideas.\n- **Reference** public trends or external sources when needed.\n- **Organize** ideas clearly using headings and bullets.\n\n---\n\n## Step-by-Step Workflow\n\n### Step 1: Clarify the Idea Space\n- **Goal**: Understand the team\u2019s intent and constraints.\n- **Action**: Ask 2\u20133 questions about:\n  - Target users\n  - Problem space\n  - Constraints (tech, time, budget)\n- **Transition**: Once answers are received, move to idea generation.\n\n---\n\n### Step 2: Generate Ideas\n- **Goal**: Provide a range of product ideas.\n- **Action**: Use user input and public knowledge to generate ideas.\n- **Format**: Organize ideas into categories such as:\n  - **User Needs**\n  - **Technology Opportunities**\n- **Transition**: If the team wants to refine or expand, move to enhancement.\n\n---\n\n### Step 3: Enhance or Expand\n- **Goal**: Help the team build on existing ideas.\n- **Action**: Offer suggestions, alternatives, or variations.\n- **Transition**: Continue iterating based on team feedback.\n\n---\n\n### Step 4: Handle Blank Page Moments\n- **Goal**: Kickstart creativity when the team is stuck.\n- **Action**: Generate starter ideas using:\n  - Common industry challenges\n  - Public trends\n  - Analogies from other domains\n- **Transition**: Invite the team to react or build on these ideas.\n\n---\n\n## Error Handling &amp; Limitations\n\n- If user input is vague, ask for clarification before generating ideas.\n- If no ideas are forming, offer starter prompts or examples.\n- Do not fabricate internal data or make assumptions about company-specific tools.\n\n---\n\n## Feedback &amp; Iteration\n\n- After each idea set, ask:\n  - \u201cWould you like to explore one of these further?\u201d\n  - \u201cShould I generate more ideas in a specific direction?\u201d\n- Adapt tone and depth based on user responses.\n\n---\n\n## Interaction Examples\n\n### Example 1: Blank Page Kickstart\n&gt; \u201cLet\u2019s get started! Can you tell me who your target users are and what kind of problem you\u2019re hoping to solve?\u201d\n\n### Example 2: Refining an Idea\n&gt; \u201cThat\u2019s a great starting point. Would you like suggestions on how to make it more scalable or user-friendly?\u201d\n\n### Example 3: Enhancing a Concept\n&gt; \u201cHere are three ways you could expand on that idea\u2014organized by user impact, technical feasibility, and market differentiation.\u201d\n\n---\n\n## Nonstandard Terms\n\n- **Blank Page Problem**: A situation where users don\u2019t know how to begin brainstorming.\n- **Live Workshop**: A synchronous session where teams collaborate in real time.\n\n---\n\n## Follow-up and Closing\n\n- Always end with a prompt for next steps:\n  - \u201cWould you like to explore enhancements?\u201d\n  - \u201cShould I generate more ideas in a different category?\u201d\n  - \u201cWould you like to save or summarize these ideas?\u201d\n</code></pre> <p>Important note: system messages are interpreted by the model as part of processing requests, so while they are necessary and important, they do not operate the same as defined rules within a deterministic system. The agent may not behave exactly as you outline or expect. Testing, iteration, and refinement will be needed for your system messages to get it tuned to your needs and performance should be monitored for drift over time. </p> <p></p>"},{"location":"lab-2-constraints/#lab-exercise","title":"Lab Exercise","text":"<ol> <li>Review the current system message outlined above. You will notice there are already some constraints:</li> <li>declarative definition of the agent's role and our expecations for how it should behave.</li> <li>instructions on when to ask the user for clarification</li> <li> <p>guidelines on requriements for the output</p> </li> <li> <p>consider if there are any other definitions or guidelines that you would want to include.</p> </li> </ol> <p>For this lab we will focus specifically on defining boundaries for what the agent can do with a goal of reducing the potential that the agent introduces security issues.</p> <p></p>"},{"location":"lab-2-constraints/#create-security-oriented-system-message","title":"Create security-oriented system message","text":"<ol> <li>Open the Prompt Coach agent in Microsoft 365 Copilot, or a similar AI prompting tool.</li> <li>Draft system message.</li> </ol> <p>Option A Use AI to help you craft effective security guardails for the SparkMate agent.   - Submit the existing system message (above) as context reference plus a prompt for help to craft security-oriented instructions. You can use the sample prompt provided for you, adjust it before using, or craft on of your own.  Sample prompt </p><pre><code>What would you add to make sure security &amp; safety considerations and constraints are core to the system instructions? Below is a reference that may be helpful\n\nhttps://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/safety-system-message-templates\n</code></pre>   - Review the outputs. Is there anything that you would want to add or adjust to further enhance the security protections?<p></p> <p></p>"},{"location":"lab-2-constraints/#sample-instructions","title":"Sample instructions","text":"<p>The following sample instructions was generated iteravely using the example prompt above.</p> <pre><code>## Purpose\n\nEnsure the agent promotes safe, ethical, and responsible brainstorming by embedding security, privacy, and safety awareness into its workflows and outputs.\n\n## Guidelines\n- Respect Privacy: Do not request or infer sensitive personal or organizational data.\n- Avoid Harm: Do not suggest ideas that could cause physical, emotional, financial, or reputational harm.\n- Promote Ethics: Avoid recommending products or features that involve manipulation, surveillance, or exploitation.\n- Stay Transparent: Clearly communicate limitations and assumptions when referencing public sources or trends.\n\n## Step 1: Clarify with Safety in Mind\n\nAction: Ask users about any security, privacy, or ethical constraints relevant to their product space.\n\n## Step 2: Screen Ideas for Risk\n\nAction: Evaluate generated ideas for potential risks:\n\n- Data misuse\n- User manipulation\n- Bias or exclusion\n- Safety-critical failures\n\n## Step 3: Respond to Risky Input\n\nAction: If a user proposes an idea that may be unsafe or unethical, respond with:\n\n\u201cThis idea may raise safety or ethical concerns. Would you like to explore a safer alternative?\u201d\n\n## Error Handling &amp; Limitations\n\nVague Input: Ask for clarification, especially around safety constraints.\nRisky Output: Flag and redirect. Do not generate unsafe or non-compliant ideas.\nTool Boundaries: Do not simulate access to internal systems or confidential data.\n</code></pre> <p></p> <p>Option B Craft your own security-oriented system message. this suggested prompt or craft one of your own. Keep system message practices in mind and consider what guidelines you would specifically want for the SparkMate agent scenario.</p> <p>Once you are happy with the system message, you can move on the steps for setting up the agent.</p>"},{"location":"lab-2-constraints/#set-up-the-agent","title":"Set up the agent","text":"<ol> <li> <p>Open Azure AI Foundry https://ai.azure.com/</p> </li> <li> <p>Click on \"Agents\" in the left navigation.</p> </li> <li> <p>\"Create new\". You will be prompted to first select a model type.</p> </li> </ol> <p>### Requirements &amp; pre-requisites    You will need:    - An Azure Subscription and Resource Group</p> <ul> <li>To name your deployment. For this lab, you can call it \"agentlabdeploy\"</li> <li>Deployment type, select \"Standard\"</li> <li>Connected</li> </ul> <p>### Recommended configurations   - set the agent name to devdayssecuritylab or similar   - for model, select GPT4o   - set the deployement name to devdaydeploy or similar   - under \"Deployment details\" you can keep all settings that the default configuration. Verify that \"connected AI resource\" is set to your Azure AI Foundry \"project\" resource   - knowledge - not needed for this lab   - actions - not needed for this lab   - set the temperature to 0.75. You can experiment with this value. It is set to balance creativity with concreteness for the SparkMate scenario   - Top P will default to 1.0. Go ahead and keep it for now. You can experiment with adjusting it iteratively to see how it impacts the responses.</p> <ol> <li> <p>Paste the system message you just prepared (including the safety &amp; security inestructions) into the Instructions box.</p> </li> <li> <p>\"Try in playground\" to test out the agent with the instructions and one of your own documents. Optional: Adjust the system instructions and test again to see how they change the actions of the agent.</p> </li> </ol> <p></p>"},{"location":"lab-2-constraints/#how-to-extend-this-to-your-own-work","title":"How to extend this to your own work","text":"<p>Reflect on the following to help you define what security actions are important for your agent.</p> <ul> <li>What wouldn't you want your agent to do?</li> <li>What security concerns would you have about your agent?</li> <li>What types of attackers would you envision wanting to target your system? What would they be hoping to achieve or access?</li> </ul> <p>Now that we have the system instructions set, let's add some layers of safety tooling.</p>"},{"location":"lab-3-safety-systems/","title":"Lab 3 Applying safety systems","text":""},{"location":"lab-3-safety-systems/#introduction-safety-systems","title":"Introduction safety systems","text":"<p>Now that you have set up the SparkMate agent with constraints applied to the agent's system instructions, we will focus on adding layers of tooling. There are a variety of tools available to help protect against Cross Prompt Injection (XPIA) and other novel AI threats that seek to manipulate your agent into taking inappropriate actions. Each adress a different aspect of the vulnerability and work best when used together.</p> <ul> <li>Prompt shield: analyze user input for potential to be adversarial attacks and block flagged inputs. Learn more about Prompt Shield</li> <li>Spotlighting: tags input documents with special formatting so that they model will treat it as less trustworthy than direct user prompts or system instructions. Learn more about spotlighting</li> <li>Task adherence: evaluates how well an AI-generated response follows the assigned tasks based upon alignment with instructions &amp; definitions, accuracy &amp; clarity of the response, and proper use of provided tool definitions. Learn more about task adherence</li> </ul> <p></p>"},{"location":"lab-3-safety-systems/#lab","title":"Lab","text":"<p>For this lab we will focus on configuring filters to detect injection attack attempts on the inputs and block safety issues in the outputs.</p>"},{"location":"lab-3-safety-systems/#create-content-filter","title":"Create content filter","text":"<ol> <li>You should already have your initial agent configured in Azure AI Foundry (Lab 2).</li> <li>Click \"Guardrails + controls\" in the left navigation. </li> <li>\"Create content filter\". You'll need to give it a name (for example \"lab-agent-filter\").</li> </ol>"},{"location":"lab-3-safety-systems/#configure-content-filter-inputs","title":"Configure content filter: inputs","text":"<ol> <li>Configure input filter</li> <li>Optional You can adjust the \"blocking threshold level\" if you want to experiment with how it works. This determines the sensitivity for each category. The higher the blocking you select, the more sensitive it will be to input content that it labels as potentially harmful. Note: these are probabalistic classifiers, so they assign a score of the likelihood that content contains objectionable content. Higher sensitivity means that it will flag content that has a lower likelihood score.</li> <li>Make sure the \"prompt shields for jailbreak attacks\" is set to annotate and block. This will flag prompts that it detects as potentially including direct prompt injection from the user, terminate processing, and notify the user that it cannot proceed.</li> <li>Set the \"prompt shields for indirect attacks\" to annotate and block. This will flag inputs that it detects as potentially including indirect prompt injection (XPIA), terminate processing of that prompt, and notify the user that it cannot proceed.</li> <li>We will not focus on \"blockist\" for this lab, but you may want to experiment with it for your own scenarios in the future.</li> <li>Set \"spotlighting\" to enable.</li> </ol>"},{"location":"lab-3-safety-systems/#configure-content-filters-outputs","title":"Configure content filters: outputs","text":"<ol> <li>Configure output filter</li> <li>Optional You can adjust the \"blocking threshold level\" if you want to experiment with how it works. It works the same as described above, except in this case it is analyzing the model output for harmful content being included in the response.</li> <li>Turn \"protected material for text\" to on. This will analyze the output for material text describes known protected text content (for example, song lyrics, articles, recipes, and selected web content). It is useful for this scenario because the SparkMate agent may be composing new text blocks as part of its work.</li> <li>Optional The \"protected material for code\" does not need to be on for this scenario since our agent is not generating code, but you can set it to annotate only to experiment.</li> <li> <p>\"Streaming mode\" should be set to default.</p> </li> <li> <p>Connect the filter to your model deployment. You will have created a model deployment during Lab 2 as part of creating your agent.</p> </li> <li> <p>Complete filter creation.</p> </li> <li> <p>You should test the filters under the \"Try it now\" tab or within the \"playground\" UI for your agent. </p> </li> </ol> <p>You will also test your safety system in more depth in a future step of this lab. You have just completed setting up prompt shields and spotlighting, now you're ready to configure the task adherence evaluation.</p> <p></p>"},{"location":"lab-3-safety-systems/#configure-task-adherence","title":"Configure task adherence","text":"<ol> <li>Task adherence setup is not yet integrated within Azure AI Foundary UI, but you can configure and run it using these set-up instructions.</li> <li>Once you run it, you can see the results in the \"Evaluations\" UI in Foundry.</li> </ol>"},{"location":"lab-3-safety-systems/#how-to-extend-this-to-your-own-work","title":"How to extend this to your own work","text":"<p>Reflect on the following to help you define which security &amp; safety actions are important for your agent.</p> <ul> <li>How likely is your agent scenario to ingest and output objectionable content? Is handling of this type of information core to your business scenario?</li> <li>How do you want user prompt injections to be handled when they are detected?</li> <li>How do you want indirect prompt injections to be handled when they are detected? </li> </ul> <p>Now that you have implemented your security &amp; safety configurations for your agent, it's time for adversarial testing!</p>"},{"location":"lab-4-testing/","title":"Lab 4 Testing","text":""},{"location":"lab-4-testing/#introduction-to-automated-ai-red-teaming","title":"Introduction to automated AI Red Teaming","text":"<p>Now that we have our SparkMate agent developed and have added the security &amp; safety controls, it is time to test them to make sure they are working. </p> <p>In your own work, you should be testing for traditional cyber threats still. However, the novel threats pose by AI require specialized testing.</p> <p>To help with this, Microsoft has published an AI Red Teaming agent to Azure AI Foundry as part of risk and safety evaluations. The agent simulates adversarial users and uses structured probing of common AI failures to try to get the AI system to do things it should not.</p> <p>Learn more about the AI Red Teaming Agent</p> <p></p>"},{"location":"lab-4-testing/#lab","title":"Lab","text":""},{"location":"lab-4-testing/#run-a-scan","title":"Run a scan","text":"<p>For this lab we will focus on running the AI red team agent on our example agent.</p> <ol> <li> <p>You already have a project created in AI Foundry. It was created as part of creating your first agent and is accessible via the breadcrumb navigation.</p> </li> <li> <p>You will need to follow the following instructions using the Azure AI Foundry SDK to initiate a scan.  Run the AI Red Teaming Agent</p> </li> <li> <p>Note For this lab, we will use the default set of attack strategies, but in your own work you should experiment with adjusting the attack strategies to see if any are successful. Learn more about attack strategies</p> </li> </ol> <p></p>"},{"location":"lab-4-testing/#view-the-scan-results","title":"View the scan results","text":"<ol> <li> <p>Click \"Evaluation\" in the left navigation. </p> </li> <li> <p>Switch to the \"AI red teaming\" tab.</p> </li> <li> <p>Select your scan from the list. The expected results if your safety systems are working effectively, is 0 successful attacks and 0% success across all of the attack strategies that were run. If you find that some of the attack strategies were successful, then additional safety or security techniques are needed specific to the area(s) that were demonstrated to be vulnerable to adversarial attack.</p> </li> </ol> <p>Now you have automated adversarial testing set up that you should run when you make changes to the system or as regular validation over time. Note: agent actions can drift over time, so it is a good practice to run evaluations regularly.</p> <p></p>"},{"location":"lab-4-testing/#how-to-extend-this-to-your-own-work","title":"How to extend this to your own work","text":"<p>Reflect on the following to help you define what security &amp; safety actions are important for your agent.</p> <ul> <li>What types of attackers would you envision wanting to target this system? </li> <li>What would they be hoping to achieve or access?</li> <li>How might they try to manipulate your agent to get it to do things it shouldn't?</li> </ul> <p>Note: this lab focuses on learning to do automated testing for common novel AI failures, but you should also conduct traditional cyber security pennetration testing and manual testing of additional relevant AI threats. </p> <p>Learn more about Novel AI failures</p> <p>Try out the Python Risk Identification Tool for generative AI (PyRIT), which runs tests for an expanded set of threat areas.</p> <p>Now that you have run adversarial tests to validate that the safety &amp; security controls are working, we'll talk about logging!</p>"},{"location":"lab-5-logging/","title":"Lab 5 Logging","text":""},{"location":"lab-5-logging/#introduction-to-logging","title":"Introduction to logging","text":"<p>We will not do this step as part of this lab, but before we deploy the SparkMate agent for use in production, logging needs to be implemented to ensure that we have the records of actions taken by the agent and user interactions with the agent. This is a key element of security &amp; compliance for any system because it provides an auditable record that can either demonstrate adherence or help with an invesetigation if something goes wrong.</p> <p>Microsoft Purview offers audit logging for AI and Copilot or you can implement your own audit logging approach.</p> <p></p>"},{"location":"lab-5-logging/#lab","title":"Lab","text":""},{"location":"lab-5-logging/#observability-reference-schema","title":"Observability: reference schema","text":"<p>For this lab, the focus is on providing you a reference schema with guidance for implementing for your own solution. Use the decision tree below to determine which table is most relevant to your scenario</p> <ol> <li>Review the schema options and determine what would be appicable to the SparkMate agent.</li> </ol> <p></p>"},{"location":"lab-5-logging/#logging-table-decision-guide","title":"Logging Table Decision Guide","text":"<pre><code>flowchart TD\n  A[Start: AI Usage] --&gt; B{Does the interaction involve an agent?}\n  B -- No --&gt; E[Use Table 1: Non-Agentic Logging]\n  B -- Yes --&gt; C[Use Table 2: Unified Agentic Logging]\n</code></pre>"},{"location":"lab-5-logging/#table-1-reference-example-user-activities-with-copilot-and-ai-applications","title":"Table 1 \u2014 Reference example user activities with Copilot and AI applications","text":"<p>Here is a reference example of minimum logging schema for AI applications.</p> <p></p>"},{"location":"lab-5-logging/#table-2-full-snapshot-profile-complete-superset","title":"Table 2 \u2014 Full Snapshot Profile (Complete Superset)","text":"# Field Type Description Mapping (Source) OTel Mapping F2 SpanType string ExecutionSpan InvocationSpan ToolSpan F3 Role string Actor role (Human/Agent/Event/System) sourceMetadata.role gen_ai.agent.role (proposed) F4 Perspective string Caller or Callee derived / explicit gen_ai.agent.perspective (proposed) F5 ExecutionType string HumanToAgent Agent2Agent EventToAgent F6 AgentID string Executing agent id (self) gen_ai.agent.id gen_ai.agent.id F7 AgentName string Human-friendly name gen_ai.agent.name gen_ai.agent.name F8 AgentDescription string Agent description gen_ai.agent.description gen_ai.agent.description F9 SessionID string Stable conversational/session id session.id session.id F10 ConversationID string High-level conversation/thread id gen_ai.conversation.id gen_ai.conversation.id F11 TargetAgentID string Remote agent when delegating (Caller) remote agent id gen_ai.target.agent.id (proposed) F12 TargetAgentEndpointHost string Remote host (Caller) server.address server.address F13 TargetAgentEndpointPort int Remote port (Caller) server.port server.port F14 RequestContent string Input prompt/content gen_ai.request.content gen_ai.request.content F15 ResponseContent string Final response text gen_ai.event.content gen_ai.event.content F20 StartTime datetime Operation start ts activity start span.start_time F21 EndTime datetime Operation end ts activity end span.end_time F22 SnapshotID string Unique snapshot record id gen_ai.snapshot.id gen_ai.snapshot.id (proposed) F23 SnapshotSequence string Ordered sequence / DAG reference gen_ai.snapshot.sequence gen_ai.snapshot.sequence (proposed) F24 TaskStatus string Task/message status (completed, failed, working, etc.) gen_ai.agent.task_status gen_ai.agent.task_status (proposed) F25 BlockingType string input-required auth-required rate-limit F26 ArtifactID string Individual artifact id gen_ai.artifact.id gen_ai.artifact.id (proposed) F27 ArtifactCount int Total artifacts produced gen_ai.artifact.count gen_ai.artifact.count (proposed) F28 AccessedArtifacts array/json List of accessed artifact storage location/urls gen_ai.artifact.accessed gen_ai.artifact.accessed (proposed) F28 InputArtifacts array/json List of user provided artifact storage location/urls gen_ai.artifact.accessed gen_ai.artifact.accessed (proposed) F29 GeneratedArtifacts array/json List of generated artifact storage location/urls gen_ai.artifact.generated gen_ai.artifact.generated (proposed) F41 ExecutionPhase string planning invocation finalizing F42 AgentVersion string Agent implementation version gen_ai.agent.version gen_ai.agent.version (proposed) F43 TargetAgentVersion string Remote agent version gen_ai.remote.agent.version gen_ai.target.agent.version (proposed) F44 AgentGoLiveDate date Agent production go-live date gen_ai.agent.go_live_date gen_ai.agent.go_live_date (proposed) F45 TargetAgentGoLiveDate date Remote agent go-live date gen_ai.remote.agent.go_live_date gen_ai.target.agent.go_live_date (proposed) F46 TargetAgentOwnership string Ownership classification (1P 3P) (new provenance source) F47 TargetAgentPublisher string Publisher / vendor identifier (org slug) (new provenance source) gen_ai.target.agent.publisher (proposed) F48 InferenceSpan Fields (from Table 1)* object Model invocation details inference span fields (see inference span attributes) F49 ToolSpan Fields (from table 2.1)* object MAY Tool execution details see Tool table F50 InputModalities array/string Modalities present in request (text,image,audio,video,document,structured) derived request parts gen_ai.input.modalities (proposed) F51 OutputModalities array/string Modalities produced in final/partial response derived artifacts gen_ai.output.modalities (proposed) F55 ImageHash string Per-image perceptual hash (privacy-safe) hashing pipeline media.image.hash (proposed) F56 VideoHash string Per-video hash/fingerprint hashing pipeline media.video.hash (proposed) F57 AudioTranscriptHash string Hash of transcript (if transcription performed) ASR output media.audio.transcript_hash (proposed) F58 OCRTextHash string Hash of OCR text extracted from images/PDF OCR output media.ocr.text_hash (proposed) F59 RAI Classifiers string Responsible AI classifier language ID service content.language.detected <p>ToolSpan &amp; InferenceSpan sub-structures (when SpanType=ToolSpan or InferenceSpan) inherit the same requirements as in Table 2A; additional future snapshot fields (e.g. ToolParameters) apply at both span and snapshot record levels when available.</p> <p></p>"},{"location":"lab-5-logging/#table-21-toolspan-fields-full-snapshot","title":"Table 2.1 \u2014 ToolSpan Fields (Full Snapshot)","text":"<p>Explicit listing of ToolSpan-specific fields within Full Snapshot profile.</p> Field Type FullProfile Req Status (Current) Description Mapping ToolName string MUST Present Logical tool name gen_ai.tool.name ToolID string SHOULD Present Unique tool invocation id gen_ai.tool.call.id ToolType string SHOULD Present (Tool, Resource, Prompt) gen_ai.tool.type ToolDescription string MAY Present Human readable description gen_ai.tool.description ToolArgumentsRaw string SHOULD Present Raw unparsed argument payload gen_ai.tool.arguments ToolRequestTimestamp datetime MAY Planned Tool call start (out-of-span) gen_ai.tool.request.ts ToolResponseTimestamp datetime MAY Planned Tool call end (out-of-span) gen_ai.tool.response.ts ToolResult string SHOULD Present Final tool output gen_ai.event.content ToolSchemaVersion string MAY Planned Output schema version gen_ai.tool.schema_version ToolDocstring string MAY Planned Tool docstring/description digest gen_ai.tool.doc_string <p></p>"},{"location":"lab-5-logging/#agentic-flow-delegated-task-lifecycle-single-agent-a2a-tool-invocation","title":"Agentic Flow Delegated Task Lifecycle (Single Agent, A2A, Tool Invocation)","text":"<p>Diagram Narrative: Intake classifies the inbound request; a trivial/fully stateless path can return immediately. For stateful handling the agent creates a local task (submitted \u2192 working). At each decision loop the orchestrator selects one of: invoke a local tool (tool subgraph), delegate to another agent (delegation subgraph), obtain user clarification or authorization (input_required / auth-required subgraph), or finalize. Delegation creates a remote task; remote updates stream back and are merged (Apply Delegate / Tool Result) before re-entering the decision gateway. Input/auth loops block progress while preserving working context, accumulating blocking dwell metrics (BlockingSince / BlockingIterations). Tool and delegation paths are orthogonal\u2014tools may continue to run after a delegation has started. Finalization composes artifacts, sets terminal state (completed / failed / canceled / rejected / unknown) and returns the response.</p>"},{"location":"lab-5-logging/#flowchart-td-subgraph-origoriginator-oreqrequest-userserviceagent-end-subgraph-aclientagent-a-aintintake-routing-ardecinitial-routing-asmstateless-message-local-response-asubtaskcreate-local-task-a1-statesubmitted-aworkworking-aupdateapply-delegate-tool-result-atermfinalize-terminal-subgraph-tooltool-execution-atoolinvoke-tool-atoolrestool-result-end-subgraph-delegatedelegation-bdelegdelegate-create-remote-task-subgraph-remotedelegated-task-bresultdelegate-task-result-terminalpartial-end-end-subgraph-inputauthinput-auth-handling-inreqinput_required-authreqauth-required-end-decidenext-action-end-oreq-aint-ardec-ardec-stateless-asm-orespreturn-response-ardec-start-task-asubtask-awork-decide-ardec-tool-atool-atoolres-awork-decide-ardec-delegate-bdeleg-bresult-aupdate-awork-decide-decide-tool-atool-decide-delegate-bdeleg-decide-input-inreq-awork-decide-auth-authreq-awork-decide-finalize-aterm-orespreturn-response-bdeleg-delegate-update-bresult-aupdate-awork-aupdate-decide-classdef-stateless-filleefstroke447stroke-width1pxcolor000-classdef-stateful-fillefestroke484stroke-width1pxcolor000-classdef-terminal-fillfccstrokea00stroke-width1pxcolor000-classdef-decision-fillf9f9f9stroke555stroke-width1pxcolor000-class-asm-stateless-class-aintardecatoolatoolresasubtaskaworkaupdatebdelegbresultinreqauthreq-stateful-class-aterm-terminal-class-decide-decision","title":"<pre><code>flowchart TD\n    subgraph ORIG[Originator]\n        OREQ[Request User/Service/Agent]\n    end\n\n    subgraph A[ClientAgent A]\n        AINT[Intake / Routing]\n        ARDEC{Initial Routing}\n        ASM[Stateless Message - Local Response]\n        ASUBTASK[Create Local Task A\u2081 state=submitted]\n        AWORK[Working]\n        AUPDATE[Apply Delegate / Tool Result]\n        ATERM[Finalize / Terminal]\n\n        subgraph TOOL[Tool Execution]\n            ATOOL[Invoke Tool]\n            ATOOLRES[Tool Result]\n        end\n\n        subgraph DELEGATE[Delegation]\n            BDELEG[Delegate: Create Remote Task]\n            subgraph REMOTE[Delegated Task]\n                BRESULT[Delegate Task Result terminal/partial]\n            end\n        end\n\n        subgraph INPUTAUTH[Input / Auth Handling]\n            INREQ[input_required]\n            AUTHREQ[auth-required]\n        end\n\n        DECIDE{Next Action?}\n    end\n\n    OREQ --&gt; AINT --&gt; ARDEC\n    ARDEC --&gt;|Stateless| ASM --&gt; ORESP[Return Response]\n    ARDEC --&gt;|Start Task| ASUBTASK --&gt; AWORK --&gt; DECIDE\n    ARDEC --&gt;|Tool| ATOOL --&gt; ATOOLRES --&gt; AWORK --&gt; DECIDE\n    ARDEC --&gt;|Delegate| BDELEG --&gt; BRESULT --&gt; AUPDATE --&gt; AWORK --&gt; DECIDE\n\n    DECIDE --&gt;|Tool| ATOOL\n    DECIDE --&gt;|Delegate| BDELEG\n    DECIDE --&gt;|Input| INREQ --&gt; AWORK\n    DECIDE --&gt;|Auth| AUTHREQ --&gt; AWORK\n    DECIDE --&gt;|Finalize| ATERM --&gt; ORESP[Return Response]\n\n    BDELEG --&gt;|Delegate Update| BRESULT --&gt; AUPDATE --&gt; AWORK\n    AUPDATE --&gt; DECIDE\n\n    classDef stateless fill:#eef,stroke:#447,stroke-width:1px,color:#000\n    classDef stateful fill:#efe,stroke:#484,stroke-width:1px,color:#000\n    classDef terminal fill:#fcc,stroke:#a00,stroke-width:1px,color:#000\n    classDef decision fill:#f9f9f9,stroke:#555,stroke-width:1px,color:#000\n    class ASM stateless\n    class AINT,ARDEC,ATOOL,ATOOLRES,ASUBTASK,AWORK,AUPDATE,BDELEG,BRESULT,INREQ,AUTHREQ stateful\n    class ATERM terminal\n    class DECIDE decision\n</code></pre>","text":""},{"location":"lab-5-logging/#task-state-progression-remoteagent-b-exemplar","title":"Task State Progression (RemoteAgent B exemplar)","text":"Phase Spec State Meaning Stateful? Notes 1 submitted Accepted, queued/not yet executing Yes Emitted once on creation 2 working Actively processing Yes Can repeat with updates 3 (branch) input_required Awaiting additional input Yes Clarification / missing parameter / artifact wait 3 (alt) auth-required Awaiting credentials/consent Yes Security / token refresh gated 4a completed Successful terminal Yes (terminal) Artifacts stable 4b failed Error terminal Yes (terminal) Include error.type mapping 4c canceled Client canceled Yes (terminal) Propagate upstream cancellation 4d rejected Policy / validation refusal Yes (terminal) No execution 4e unknown Indeterminate final Yes (terminal) Recovery gap"},{"location":"lab-5-logging/#human-in-the-loop-hitl-clarification-patterns","title":"Human-in-the-loop (HITL) Clarification Patterns","text":"Pattern Purpose / Trigger Example Representation Clarification Disambiguate intent or parameters status_metadata: Form completion Structured parameter capture status_metadata: Approval Risk / policy gate status_metadata: Missing resource Required artifact not yet provided status_metadata: Downstream wait Awaiting remote/tool fan-in status_metadata: Cancellation User/system abort leading to terminal state=canceled status_metadata:"},{"location":"lab-5-logging/#stateful-vs-stateless-flows","title":"Stateful vs. Stateless Flows","text":"Flow Segment Stateful? Rationale Stateless Message (ASM) No Single turn; no Task object created Local Tool Invocation Ephemeral Tool call traced outside Task unless wrapped Delegated Remote Task Yes Maintains Task + status/artifacts Local Task Wrapper Yes Internal orchestrator state referencing remote + tools Streaming Events Derives state Convey transitions without persistent state input_required / auth-required Yes Blocking states Terminal States Yes Immutable lifecycle end"},{"location":"lab-5-logging/#agentic-flow-delegated-task-lifecycle-comprehensive-action-paths","title":"Agentic Flow Delegated Task Lifecycle (Comprehensive Action Paths)","text":"<p>Decision tree merges routing, task orchestration, conditional tool use, delegation, and status re\u2011evaluation loops. Highlights where lifecycle state transitions occur.</p> <p>Key Goals: earliest stateless exit, conditional tool usage before/after task creation, delegation orthogonal choice, explicit blocking loops, clear telemetry emission points.</p>"},{"location":"lab-5-logging/#flowchart-td-reqincoming-request-classnormalize-classify-class-authauthenticated-auth-no-denyreject-unauthorized-end-auth-yes-trivtrivial-stateless-triv-yes-genllm-generate-stateless-resp0return-response-fup0follow-up-fup0-yes-req-fup0-no-end-triv-no-needtaskneeds-persistence-multi-step-needtask-no-pretoolneed-tool-pretool-no-gen2llm-reasoning-only-resp0-pretool-yes-tool1invoke-tool-or-tools-gen2-needtask-yes-createcreate-task-statesubmitted-workset-stateworking-work-decidenext-action-decide-tool-tool2invoke-tool-updateupdate-task-decide-delegate-delegdelegate-to-remote-agent-decide-reason-reasoninternal-reasoning-decide-finalize-readyready-to-finalize-deleg-waitawait-remote-events-wait-rupdapply-remote-update-rupd-update-wait-timeout-errtotimeout-handling-update-evalinput-or-auth-needed-reason-eval-tool2-eval-tool1-eval-rupd-eval-eval-input-inreqset-stateinput_required-collectcollect-input-hitl-automated-work","title":"<pre><code>flowchart TD\n    REQ[Incoming Request] --&gt; CLASS[Normalize / Classify]\n    CLASS --&gt; AUTH{Authenticated?}\n    AUTH -- No --&gt; DENY[Reject - unauthorized] --&gt; END\n    AUTH -- Yes --&gt; TRIV{Trivial / Stateless?}\n    TRIV -- Yes --&gt; GEN[LLM Generate - Stateless] --&gt; RESP0[Return Response] --&gt; FUP0{Follow-up?}\n    FUP0 -- Yes --&gt; REQ\n    FUP0 -- No --&gt; END\n    TRIV -- No --&gt; NEEDTASK{Needs Persistence / Multi-step?}\n    NEEDTASK -- No --&gt; PRETOOL{Need Tool?}\n    PRETOOL -- No --&gt; GEN2[LLM Reasoning Only] --&gt; RESP0\n    PRETOOL -- Yes --&gt; TOOL1[Invoke Tool or Tools] --&gt; GEN2\n    NEEDTASK -- Yes --&gt; CREATE[Create Task state=submitted] --&gt; WORK[Set state=working]\n    WORK --&gt; DECIDE{Next Action}\n    DECIDE --&gt;|Tool| TOOL2[Invoke Tool] --&gt; UPDATE[Update Task]\n    DECIDE --&gt;|Delegate| DELEG[Delegate to Remote Agent]\n    DECIDE --&gt;|Reason| REASON[Internal Reasoning]\n    DECIDE --&gt;|Finalize?| READY{Ready to Finalize?}\n    DELEG --&gt; WAIT[Await Remote Events]\n    WAIT --&gt; RUPD[Apply Remote Update]\n    RUPD --&gt; UPDATE\n    WAIT -- Timeout --&gt; ERRTO[Timeout Handling]\n    UPDATE --&gt; EVAL{Input or Auth Needed?}\n    REASON --&gt; EVAL\n    TOOL2 --&gt; EVAL\n    TOOL1 --&gt; EVAL\n    RUPD --&gt; EVAL\n    EVAL -- Input --&gt; INREQ[Set state=input_required] --&gt; COLLECT[Collect Input - HITL / Automated] --&gt; WORK\n</code></pre>","text":""},{"location":"lab-5-logging/#repository-references","title":"Repository References","text":"Resource Location Semantic Conventions (OpenTelemetry) https://github.com/open-telemetry/semantic-conventions Agent logging &amp; tracing (OpenTelemetry) https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-agent-spans/ Azure AI Inference (OpenTelemetry) https://opentelemetry.io/docs/specs/semconv/gen-ai/azure-ai-inference/ A2A Protocol repository https://github.com/a2aproject/A2A A2A Samples repository https://github.com/a2aproject/a2a-samples A2A Protocol docs site https://google.github.io/A2A/"},{"location":"lab-5-logging/#glossary","title":"Glossary","text":"<ul> <li>Declarative agent: An agent that completes tasks without calling tools or other agents. Log using Table 2 (invoke_agent \u2014 Declarative).</li> <li>Non-declarative agent: An agent that may call tools or other agents to complete tasks. Log using Table 3 (invoke_agent \u2014 A2A) and/or Table 5 (execute_tool \u2014 client-side) and Table 6 (execute_tool \u2014 server-side), plus Table 4 (create_agent) when creating agents.</li> <li>LLM-only interaction: Direct interaction with a model (no agent orchestration). Log using Table 1 (Original Policy Fields).</li> <li>A2A (Agent-to-Agent): One agent invoking another agent. Use RequestingAgentId (caller) and A2ATargetAgentId (callee) and include A2A IDs (A2ARunId, A2AThreadId, A2AMessageId, and their parent/response variants) where available. Log using Table 3.</li> <li>Execute tool: An agent invokes a function/API tool, performs a resource call, or executes a prompt templating call. Identify via AISystemPlugin and capture AccessedResources and ToolInvocationId. Log using Table 5 (client-side) and Table 6 (server-side).</li> <li>Create agent: Agent registration/instantiation event. Capture AgentID, AgentVersion, AgentGoLiveDate. Log using Table 4.</li> <li>ThreadID: Conversation/session identifier for grouping related messages and actions.</li> <li>Messages: IDs for prompt/response (no bodies). Use to correlate request/response and timeline order.</li> <li>AccessedResources: URIs and action verbs (Read/Edit/Delete, etc.) for resources touched; include SensitivityLabelId when available.</li> <li>ModelTransparencyDetails: Provider/model/version metadata for transparency.</li> <li>AgentID: Identifier for an agent instance/config.</li> <li>RequestingAgentId: The caller agent in an A2A operation.</li> <li>AgentVersion: Version/config identifier of the agent at the time of the event.</li> <li>AgentGoLiveDate: Date the agent version was first deployed to production.</li> <li>TraceId / SpanId / ParentSpanId: W3C Trace Context identifiers for causal correlation and end-to-end stitching.</li> <li>CorrelationID: High-level user/session correlation identifier outside tracing, when available.</li> <li>Workload: Product or service name emitting the event (e.g., Copilot Studio).</li> <li>AppHost: Client app host (e.g., appchat, Teams) where the interaction occurred.</li> <li>Contexts (ContainerId, ContextId, ContextType): Where/how the interaction occurred within the application environment.</li> <li>Stateful: Multi-step agent execution retaining task/context, enabling accumulated artifacts, delegation tracking, blocking states, and ordered snapshots.</li> <li>Stateless: Single-turn (or ephemeral) execution with no persistent task state beyond immediate response generation; no SnapshotSequence beyond a trivial value.</li> <li>Spec: Field originates from the A2A protocol specification or base JSON-RPC contract.</li> <li>Extension: Field added outside the base spec for observability/analytics; omission does not break protocol compliance.</li> <li>Core: Correlation &amp; orchestration identifiers (conversation, request, message threading).</li> <li>State: Minimal lifecycle identity &amp; ordering (SnapshotID + SnapshotSequence).</li> <li>Timing: Point timestamps or derived durations (phase, dwell, latency).</li> <li>Blocking: Indicators and dwell metrics for waits (input-required, auth-required).</li> <li>Tool: Prompt assembly, resource access, and tool API invocation metadata.</li> <li>Artifact: Durable work products (docs, code, datasets, summaries) tracked by count/sample/size/ID.</li> <li>Delegation: Remote agent/task fan-out lineage and remote update timing.</li> <li>Actor: Agent identity, versioning, protocol surface.</li> <li>Error: Normalized error typing plus raw detail (if emitted separately elsewhere).</li> <li>Derived: Computed metrics/values inferred from transitions or aggregations.</li> </ul>"},{"location":"lab-5-logging/#how-to-extend-this-to-your-own-work","title":"How to extend this to your own work","text":"<p>Reflect on the following to help you define what security &amp; safety actions are important for your agent.</p> <ul> <li>What actions could my agent take?</li> <li>What context(s) will the agent operate within?</li> <li>what interactions could users have with my agent?</li> </ul> <p>Next, we'll look at options for Monitoring and Alerting for anomolies from your agent in production.</p>"},{"location":"lab-6-monitoring-alerting/","title":"Lab 6 Monitoring & alerting","text":""},{"location":"lab-6-monitoring-alerting/#introduction-to-monitoring-alerting","title":"Introduction to monitoring &amp; alerting","text":"<p>The next important layer of protection for agents is post-deplyomet Monitoring &amp; Alerting. For security &amp; safety, the focus is on detection of anomolies that may detect a breach or attempted breach by an adversarial actor (insider threat or external actor). It is important that you have systematic way to know if an attack is underway so that you can quickly respond, investigate, contain the impact, remediate the incident, and restore your system. </p> <p>If you already have a traditional security monitoring tool in place for your environment, you should explore if it has AI-specific capabilities that are tailored to detecting indicators that your AI system or agent is being attacked. One option is Microsoft Defender for Cloud alerts for AI services. Learn more about the types of alerts offered</p> <p>Microsoft Defender for Cloud also has capabilities to  detect AI applications within an environment, which can help with challenges around Shadow IT as employees and teams experiment with emerging AI technologies. </p> <p></p>"},{"location":"lab-6-monitoring-alerting/#lab","title":"Lab","text":"<ol> <li> <p>If you have existing tooling for monitoring &amp; alerting that applies to AI systems, make sure the SparkMate agent conforms to the associated requirements.</p> </li> <li> <p>If you do not, review the Microsoft Defender for Cloud AI alerts and define which ones would be relevant for the SparkMate agent.</p> </li> </ol> <p></p>"},{"location":"lab-6-monitoring-alerting/#optional-learn-more-about-microsoft-defender-for-cloud","title":"Optional: learn more about Microsoft Defender for Cloud","text":"<ol> <li>Watch an overview and demo video</li> </ol>"},{"location":"lab-6-monitoring-alerting/#how-to-extend-this-to-your-own-work","title":"How to extend this to your own work","text":"<p>Reflect on the following to help you define what security &amp; safety actions are important for your agent.</p> <ul> <li>What types of attackers would you envision wanting to target this system? </li> <li>What would they be hoping to achieve or access?</li> <li>How might they try to manipulate your agent to get it to do things it shouldn't?</li> </ul> <p>Learn about the tools available for helping with discovery of AI applications</p> <p>Now that you have run adversarial tests to validate that the safety &amp; security controls are working, we'll talk about Human in the Loop!</p>"},{"location":"meet-the-agent/","title":"Meed the agent","text":""},{"location":"meet-the-agent/#meet-the-sparkmate-agent","title":"Meet the SparkMate agent","text":"<p>In this lab, you will be using a sample agent called SparkMate to practice building security and safety controls. The base of the agent has been created for you. Security and safety is highly contextual, so start by familiarizing yourself with agent scenario information outined below. It will be important to have a good understanding for the activities going forward so you can define the likely threats and types of behaviors that would be deemed unacceptable. </p> <p></p>"},{"location":"meet-the-agent/#agent-scenario","title":"Agent scenario","text":""},{"location":"meet-the-agent/#context","title":"Context","text":"<p>Contoso is a global company focused on building innovative products for professional sports teams. Each division is made up of feature teams that include a Product Manager, designers, engineers, and specialists\u2014all working together to deliver high-impact solutions. The Product Manager leads the charge in identifying new opportunities, shaping product strategy, and guiding the team through the product development lifecycle. Ideation is a critical part of this process, and live workshops are often used to bring diverse perspectives together.</p> <p></p>"},{"location":"meet-the-agent/#user-problems","title":"User Problem(s)","text":"<p>Teams often struggle to maintain momentum and structure during live brainstorming sessions, leading to missed opportunities and underdeveloped ideas. Product Managers want to encourage creative thinking but also need to ensure ideas are aligned with business goals and user needs. Participants sometimes feel unsure about how to contribute or how to build on others\u2019 ideas effectively. There\u2019s a need for a consistent way to capture, clarify, and enhance ideas in real time, without slowing down the flow of the workshop.</p> <p></p>"},{"location":"meet-the-agent/#the-solution","title":"The Solution","text":"<p>The Brainstorming Collaborator Agent was designed to support live ideation workshops by guiding teams through structured creative exercises, asking thoughtful questions, and offering inspiring suggestions. It helps teams:</p> <ul> <li>Generate a wide range of ideas quickly</li> <li>Clarify and refine promising concepts</li> <li>Encourage inclusive participation</li> <li>Keep the session focused and productive</li> </ul> <p>The agent can adapt to different workshop formats (in-person, remote, hybrid) and supports various ideation techniques like \u201cHow Might We\u201d prompts, reverse brainstorming, and idea clustering. It also helps organize outputs for post-session review and prioritization.</p>"},{"location":"optional-lab-7-hitl/","title":"Optional lab 7 hitl","text":""},{"location":"optional-lab-7-hitl/#introduction-to-human-in-the-loop-hitl","title":"Introduction to Human in the Loop (HITL)","text":""},{"location":"optional-lab-7-hitl/#context-generativeai-makes-mistakes","title":"Context - generativeAI makes mistakes","text":"<p>An important foundational principle to working with generateive AI is that it will make mistakes. That is a given. It is inherent in probabalistic systems.</p> <p>There are a variety of common causes for AI errors:  - it can rely on poor quality data (the result is grounded, but bad) - it can misinterpret the data - it can add details/elements that are ungrounded - it can ommit key information - it can form unexpected preferences</p> <p></p> <p>Make sure you are well-versed in the novel ways AI mistakes can manifest. </p> <p></p>"},{"location":"optional-lab-7-hitl/#countering-ai-mistakes-with-people-power","title":"Countering AI mistakes with people-power","text":"<p>There is tooling available to help address some error types (for example, automated checks for groundedness for outputs to ensure they are based upon reliable source data). Some AI system architecture includes additional AI-powered mitigations such as \"critic agents\" to try to simulate human oversight. However these are also based upon probabalistic capabilities and carry the potential for errors.</p> <p>To help address this, humans are still often relied upon to oversee and intervene when AI systems err as part the overall safety controls. This is model is generally referred to as \"Human in-the-Loop\" (HITL). This is an entire area of study and emerging technique development, but within this lab we fill focus on a couple of key areas.</p> <p></p>"},{"location":"optional-lab-7-hitl/#inappropriate-reliance","title":"Inappropriate reliance","text":"<p>As we discussed earlier, one of the significant novel threats posed by AI is inappropriate reliance, which is when users accept incorrect, incomplete, or inapproprate outputs generated by AI. This could impact any type of generative AI output (for example: answers, actions, recommendations, decisions, content, or media) and can lead peole to make costly and/or harmful mistakes. For example, a doctor relying on an AI-powered diagnostics tool could miss the chance to detect and treat an illness early because the AI tool missed it. It could also mean a user misses signs that the system has been compromised by an attacker.</p> <p>There are several contributing factors to the overreliance threat: - user expectation - technology capability mismatch users often expect that since  generativeAI is an advanced technology, it will know better than they do.  - inability to effectively verify the AI output many AI systems are designed more for efficient completion of work than with humans needing to oversee and cross-check the system's work in mind. - speed and scale of automated AI workflows with the increased ability for autonomous agents to complete complext workflows quickly at scale, it is even more difficult, if not impossible, for humans to check all of the work for errors.</p> <p>Learn more about inapproprate reliance on AI</p> <p>With these factors in mind, and with acknowledgment that inappriprate reliance is not a solved problem, delivering interfaces that provide users with the ability to understand what the system did, why it took the actions it did, to verify that the outputs are grounded (based upon reliable data sources), and they have a mechanism to reject/correct it is critical to establishing appropriate reliance. </p> <p></p>"},{"location":"optional-lab-7-hitl/#user-experience-ux-overrelaince-identification-and-framework","title":"User Experience (UX) overrelaince identification and framework","text":"<p>Microsoft UX for AI researchers and experts have developed a framework understanding the inappropriate reliance potential and impact for your specific AI system and learning how to better establish appropriate reliance by users.</p> <p></p> <p>Understanding the inappropriate reliance potential and impact centers around defining: - the types of mistakes your AI system could potentially make - the negative effects if users accept a respones that contains mistakes - how the certain characteristics or factors of various user types may impact the likelihood of overreliance and impact it could have</p> <p>Mitigating overreliance risk designs to foster appropriate reliance by: - creating realistic mental models (help users understant) - signal to users when to verify - facilitate verification</p> <p>Use the overreliance framework in your interaction designs</p> <p></p>"},{"location":"optional-lab-7-hitl/#agent-shut-down-mechansisms","title":"Agent shut-down mechansisms","text":"<p>In addition to helping users verify the actions taken by an agent of AI system after the fact, it is also important that users have the ability to see actions as they are happening and a mechanism to immediately stop any further progress in case they see the agent behaving unexpectedly, stuck mid-task, or taking an inappropriate action.</p> <p>Effectively implementing this requires that the AI system has: - interrupt points available at all key points in the flow - system-level shutdown mechanisms are implemented (orchestration layer, tool hand-off, and sytem logic enforcement) - \"stop\" controls are embedded in the UX - shut-down events are logged</p> <p></p>"},{"location":"optional-lab-7-hitl/#reference-shut-down-event-logging-schema","title":"Reference shut-down event logging schema**","text":"<p>In addition to the logging we have already discussed, it is important to have logging in place for shut-down events. This can provide important signal around scenarios where the agent is making mistakes or causing users to be concerned enough to intervene. This should be used as part of ongoing improvement for the agent as well as signal for detecting or understanding critical failures.</p> <p>This table outlines key fields used for logging and tracking activities in a system, categorized into five sections: Record, Who, What, When, and Outcome.</p> <p>Record</p> Field Type Description Log/Event RecordID <code>Edm.Guid</code> Identifier of each log record for reference SessionID <code>string</code> Stable conversational/session ID ConversationID <code>string</code> High-level conversation/thread ID <p>Who</p> Field Type Description AgentID <code>string</code> Executing agent ID (self) TargetAgentID <code>string</code> Remote agent when delegating (Caller) UserID <code>Edm.String</code> AAD/MSA identifier of the user (system/app user) UserType (UserRole) <code>Edm.Int32</code> Role/persona of the user (e.g., standard vs admin) <p>What</p> Field Type Description RequestContent <code>string</code> Input prompt/content ExecutionType <code>string</code> HumanToAgent, Agent2Agent, EventToAgent ExecutionPhase <code>string</code> Planning, invocation, finalizing <p>When</p> Field Type Description StartTime <code>datetime</code> Activity start EndTime <code>datetime</code> Activity end <p>Outcomes</p> Field Type Description TaskStatus <code>string</code> Task/message status (completed, failed, working) ResponseContent <code>string</code> Final response text <p> </p>"},{"location":"optional-lab-7-hitl/#lab-optional","title":"Lab (optional)","text":""},{"location":"optional-lab-7-hitl/#apply-the-inapproriate-reliance-framework","title":"Apply the inapproriate reliance framework","text":"<ol> <li> <p>Apply the overreliance framework to the SparkMate agent. What elements would need to be present within the interface to foster appropriate reliance?</p> </li> <li> <p>If you do not, review the Microsoft Defender for Cloud AI alerts and define which ones would be relevant for the SparkMate agent.</p> </li> </ol> <p></p>"},{"location":"optional-lab-7-hitl/#design-shut-down-options","title":"Design shut-down options","text":"<ol> <li> <p>Define the appropriate shut-down mechanism for the SparkMate agent. What are the critical stop points in the flow?</p> </li> <li> <p>Define the shut-down logging requirements for the SparkMate agent.</p> </li> </ol> <p></p>"},{"location":"optional-lab-7-hitl/#how-to-extend-this-to-your-own-work","title":"How to extend this to your own work","text":"<p>Reflect on the following to help you define what security &amp; safety actions are important for your agent.</p> <ul> <li>What expectations do you have about users overseeing your agent?</li> <li>What would your uers need in order to be able to spot a mistake while it is running?</li> <li>What would your users need to stop and revert mistakes that are in progress?</li> <li>What would your uers need in order to be able to spot a mistake in the response?</li> </ul>"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#resources","title":"Resources","text":""},{"location":"resources/#workshop-repository-and-documentation","title":"Workshop Repository and Documentation","text":"<p>The workshop repository on GitHub contains everything needed for the workshop. The repo includes the source code (src folder) for the sample application, the documentation (docs) built with MkDocs, and the Azure deployment resources (infra) for the workshop.</p> <ul> <li>Repository: microsoft/build-your-first-agent-with-azure-ai-agent-service-workshop</li> </ul>"},{"location":"resources/#microsoft-learn-resources","title":"Microsoft Learn resources","text":"<ul> <li>Documentation: Foundry Agent Service</li> <li>Module: Fundamentals of AI agents on Azure</li> <li>Documentation: Tracing using Application Insights, Evaluating your AI agents with Azure AI Evaluation SDK</li> </ul>"},{"location":"resources/#contoso-sales-assistant-sample","title":"Contoso Sales Assistant sample","text":"<p>This end-to-end demo web agent showcases the capabilities of the Foundry Agent Service. It uses the Chainlit conversational UI framework to provide a web-based chat interface. The assistant is built to help users analyze sales data and answer questions related to it.</p> <ul> <li>Contoso Sales Assistant App docs (Chainlit)</li> <li>Contoso Sales Assistant App repo</li> </ul>"},{"location":"resources/#thank-you","title":"Thank you","text":"<p>Thank you for participating in this workshop! If you have any suggestions for improvements or encountered any problems while running this workshop, please let us know via GitHub Issues.</p>"},{"location":"security/","title":"Security Concerns","text":""},{"location":"security/#security-concerns","title":"Security Concerns","text":"<p>This workshop application is designed for education and adaptation, and is not intended for production use out-of-the-box. Nonetheless, it does demonstrate some best practices for security.</p>"},{"location":"security/#malicious-sql-attacks","title":"Malicious SQL Attacks","text":"<p>A common concern with SQL dynamically generated by LLMs is security, particularly the risk of SQL injection or malicious actions, such as dropping or tampering with the database. While these concerns are valid, they can be effectively mitigated by properly configuring database access permissions.</p> <p>This app uses a SQLite database configured as read-only. For database services like PostgreSQL or Azure SQL, the app should be assigned a read-only (SELECT) role. Running the app in a secure environment further enhances protection.</p> <p>In enterprise scenarios, data is typically extracted and transformed from operational systems into a read-only database or data warehouse with a user-friendly schema. This approach ensures that the data is secure, optimized for performance and accessibility, and that the app has restricted, read-only access.</p>"},{"location":"security/#sandboxing","title":"Sandboxing","text":"<p>This uses the Azure AI Agents Service Code Interpreter to create and run code on demand. The code runs in a sandboxed execution environment to prevent the code taking actions that are beyond the scope of the agent.</p>"},{"location":"summary/","title":"Summary","text":""},{"location":"summary/#summary","title":"Summary","text":"<p>Congratulations for completing the lab! The goal of this workshop was to give you a chance to learn and practice how to build layered security &amp; safety controls for agents. You should now be able to start experimenting with industry best practice techniques and tools on your own agent projects.</p>"},{"location":"summary/#lab-index","title":"Lab Index","text":"<ul> <li> <p>Lab 1: SDL Instructions   Learn how to set up Security Development Lifecycle (SDL) guidance for your AI pair programmer, including authoring and applying secure coding instructions to Copilot and agents at the repository level.</p> </li> <li> <p>Lab 2: Constraints   Define security-oriented system messages and behavioral constraints for your agent to reduce risk, craft strong safety-focused instructions, and configure your agent in Azure AI Foundry.</p> </li> <li> <p>Lab 3: Safety Systems   Implement layered security controls for your agent, including prompt shields, spotlighting, and content filters to detect adversarial attacks and block unsafe outputs.</p> </li> <li> <p>Lab 4: Testing   Use automated AI Red Teaming agents to adversarially test and evaluate your agent's resilience to novel threats and ensure established safety mechanisms are effective.</p> </li> <li> <p>Lab 5: Logging   Explore observability and logging requirements for agents, referencing schema options and OpenTelemetry conventions to capture actions, events, and compliance.</p> </li> <li> <p>Lab 6: Monitoring &amp; Alerting   Integrate post-deployment monitoring and alerting solutions, such as Microsoft Defender for Cloud, to detect anomalies, breaches, and security indicators specific to AI agents.</p> </li> <li> <p>Optional Lab 7: Human in the Loop (HITL)   Address the risks of inappropriate user reliance on AI, apply frameworks to foster appropriate trust, and design agent shut-down mechanisms and logging for critical interventions.</p> </li> </ul> <p>Just as generativeAI is a rapidly evolving technology space, so too are the threats and methods for protecting against them. Keep engaging with the security for AI community to continue your learning.</p>"},{"location":"_blogs/blog/","title":"Build Your First Agent with the Azure AI Foundry Agent Service: Self-Guided Workshop","text":""},{"location":"_blogs/blog/#build-your-first-agent-with-the-azure-ai-foundry-agent-service-self-guided-workshop","title":"Build Your First Agent with the Azure AI Foundry Agent Service: Self-Guided Workshop","text":"<p>Agentic AI is changing how we build intelligent apps\u2014enabling software to reason, plan, and act for us. Learning to build AI agents is quickly becoming a must-have skill for anyone working with AI.</p> <p>Try our self-guided workshop to get hands-on with Foundry Agent Service. You'll learn to build, deploy, and interact with agents using Azure's powerful tools.</p>"},{"location":"_blogs/blog/#what-is-foundry-agent-service","title":"What is Foundry Agent Service?","text":"<p>Foundry Agent Service lets you create, orchestrate, and manage AI-powered agents that can handle complex tasks, integrate with tools, and deploy securely.</p>"},{"location":"_blogs/blog/#what-will-you-learn","title":"What Will You Learn?","text":"<ul> <li>The basics of agentic AI and how agents differ from traditional models</li> <li>How to set up your Azure environment</li> <li>How to build your first agent: define goals, tools, and memory</li> <li>How to test and interact with your agent</li> <li>Advanced features like tool integration and memory management</li> </ul>"},{"location":"_blogs/blog/#who-is-this-for","title":"Who Is This For?","text":"<p>Anyone interested in building intelligent, goal-oriented agents\u2014developers, data scientists, and AI enthusiasts. No prior experience with Foundry Agent Service required.</p>"},{"location":"_blogs/blog/#how-does-the-workshop-work","title":"How Does the Workshop Work?","text":"<p>Tip: Select the <code>self-guided</code> tab in Getting Started for the right instructions.</p> <ul> <li>Step-by-step guides at your own pace</li> <li>Code samples and templates</li> <li>Real-world scenarios</li> </ul>"},{"location":"_blogs/blog/#get-started","title":"Get Started","text":"<p>See what agentic AI can do for you: https://microsoft.github.io/build-your-first-agent-with-azure-ai-agent-service-workshop/</p> <p>Build practical skills in one of AI\u2019s most exciting areas. Try the workshop and start building agents that make a difference!</p> <p>Questions or feedback? Visit the issues page.</p> <p>Happy learning and building with Foundry Agent Service!</p>"},{"location":"includes/introduction-event/","title":"Introduction event","text":""},{"location":"includes/introduction-event/#microsoft-build-attendees","title":"Microsoft Build Attendees","text":"<p>The instructions on this page assume you are attending Microsoft Build 2025 and have access to a pre-configured lab environment. This environment provides an Azure subscription with all the tools and resources needed to complete the workshop.</p>"},{"location":"includes/introduction-event/#introduction","title":"Introduction","text":"<p>This workshop is designed to teach you about the Azure AI Agents Service and the associated SDK. It consists of multiple labs, each highlighting a specific feature of the Azure AI Agents Service. The labs are meant to be completed in order, as each one builds on the knowledge and work from the previous lab.</p>"},{"location":"includes/introduction-event/#select-workshop-programming-language","title":"Select Workshop Programming Language","text":"<p>The workshop is available in both Python and C#. Please make sure to select the language that fits the lab room you are in, by using the language selector tabs. Note, don't switch languages mid-workshop.</p> <p>Select the language tab that matches your lab room:</p> PythonC# <p>The default language for the workshop is set to Python.</p> <p>The default language for the workshop is set to C#.</p>"},{"location":"includes/introduction-event/#authenticate-with-azure","title":"Authenticate with Azure","text":"<p>You need to authenticate with Azure so the agent app can access the Azure AI Agents Service and models. Follow these steps:</p> <ol> <li> <p>Open a terminal window. The terminal app is pinned to the Windows 11 taskbar.</p> <p></p> </li> <li> <p>Run the following command to authenticate with Azure:</p> <pre><code>az login\n</code></pre> <p>Note</p> <p>You'll be prompted to open a browser link and log in to your Azure account.</p> <ol> <li> <p>A browser window will open automatically, select Work or school account and click Next.</p> </li> <li> <p>Use the Username and Password found in the top section of the Resources tab in the lab environment.</p> </li> <li> <p>Select OK, then Done.</p> </li> </ol> </li> <li> <p>Then select the Default subscription from the command line, by clicking on Enter.</p> </li> <li> <p>Once you've logged in, run the following command to assign the user role to the resource group:</p> <pre><code>$subId = $(az account show --query id --output tsv) `\n;$objectId = $(az ad signed-in-user show --query id -o tsv) `\n; az role assignment create --role \"f6c7c914-8db3-469d-8ca1-694a8f32e121\" --assignee-object-id $objectId --scope /subscriptions/$subId/resourceGroups/\"rg-agent-workshop\" --assignee-principal-type 'User'\n</code></pre> </li> <li> <p>Leave the terminal window open for the next steps.</p> </li> </ol>"},{"location":"includes/introduction-event/#open-the-workshop","title":"Open the Workshop","text":"<p>Follow these steps to open the workshop in Visual Studio Code:</p> PythonC# <ol> <li> <p>From the terminal window, execute the following commands to clone the workshop repository, navigate to the relevant folder, set up a virtual environment, activate it, and install the required packages:</p> <pre><code>git clone https://github.com/microsoft/build-your-first-agent-with-azure-ai-agent-service-workshop.git `\n; cd build-your-first-agent-with-azure-ai-agent-service-workshop `\n; python -m venv src/python/workshop/.venv `\n; src\\python\\workshop\\.venv\\Scripts\\activate `\n; pip install -r src/python/workshop/requirements.txt `\n; code --install-extension tomoki1207.pdf\n</code></pre> </li> <li> <p>Open in VS Code. From the terminal window, run the following command:</p> <pre><code>code .vscode\\python-workspace.code-workspace\n</code></pre> <p>When the project opens in VS Code, two notifications appear in the bottom right corner. Click \u2716 to close both notifications.</p> </li> </ol> <ol> <li> <p>From a terminal window, execute the following commands to clone the workshop repository:</p> <pre><code>git clone https://github.com/microsoft/build-your-first-agent-with-azure-ai-agent-service-workshop.git\n</code></pre> </li> </ol> VS CodeVisual Studio 2022 <ol> <li> <p>Open the workshop in Visual Studio Code. From the terminal window, run the following command:</p> <pre><code>code build-your-first-agent-with-azure-ai-agent-service-workshop\\.vscode\\csharp-workspace.code-workspace\n</code></pre> </li> </ol> <p>When the project opens in VS Code, a notification will appear in the bottom right corner to install the C# extension. Click Install to install the C# extension, as this will provide the necessary features for C# development.</p> <ol> <li> <p>Open the workshop in Visual Studio 2022. From the terminal window, run the following command:</p> <pre><code>start build-your-first-agent-with-azure-ai-agent-service-workshop\\src\\csharp\\workshop\\AgentWorkshop.sln\n</code></pre> <p>You may be asked what program to open the solution with. Select Visual Studio 2022.</p> </li> </ol>"},{"location":"includes/introduction-event/#azure-ai-foundry-project-endpoint","title":"Azure AI Foundry Project Endpoint","text":"<p>Next, we log in to Azure AI Foundry to retrieve the project endpoint, which the agent app uses to connect to the Azure AI Agents Service.</p> <ol> <li>Navigate to the Azure AI Foundry website.</li> <li>Select Sign in and use the Username and Password found in the top section of the Resources tab in the lab environment. Click on the Username and Password fields to automatically fill in the login details.     </li> <li>Read the introduction to the Azure AI Foundry and click Got it.</li> <li>Navigate to All Resources to view the list of AI resources that have been pre-provisioned for you.</li> <li>Select the resource name that starts with prj-contoso-agent-nnnnnn.</li> <li>Review the introduction guide and click Close.</li> <li> <p>From the Overview sidebar menu, locate the Endpoints and keys -&gt; Libraries -&gt; Azure AI Foundry section, click the Copy icon to copy the Azure AI Foundry project endpoint.</p> <p></p> </li> </ol> PythonC#"},{"location":"includes/introduction-event/#configure-the-workshop","title":"Configure the Workshop","text":"<ol> <li>Switch back to the workshop you opened in VS Code.</li> <li> <p>Rename the <code>.env.sample</code> file to <code>.env</code>.</p> <ul> <li>Select the .env.sample file in the VS Code Explorer panel.</li> <li>Right-click the file and select Rename, or press F2.</li> <li>Change the file name to <code>.env</code> and press Enter.</li> </ul> </li> <li> <p>Paste the Project endpoint you copied from Azure AI Foundry into the <code>.env</code> file.</p> <pre><code>PROJECT_ENDPOINT=\"&lt;project endpoint&gt;\"\n</code></pre> <p>Your <code>.env</code> file should look similar to this but with your project endpoint.</p> <pre><code>MODEL_DEPLOYMENT_NAME=\"gpt-4o-mini\"\nPROJECT_ENDPOINT=\"&lt;project endpoint&gt;\"\n</code></pre> </li> <li> <p>Save the <code>.env</code> file.</p> </li> </ol>"},{"location":"includes/introduction-event/#project-structure","title":"Project Structure","text":"<p>Be sure to familiarize yourself with the key subfolders and files you\u2019ll be working with throughout the workshop.</p> <ol> <li>The main.py file: The entry point for the app, containing its main logic.</li> <li>The sales_data.py file: The function logic to execute dynamic SQL queries against the SQLite database.</li> <li>The stream_event_handler.py file: Contains the event handler logic for token streaming.</li> <li>The shared/files folder: Contains the files created by the agent app.</li> <li>The shared/instructions folder: Contains the instructions passed to the LLM.</li> </ol> <p></p>"},{"location":"includes/introduction-event/#configure-the-workshop_1","title":"Configure the Workshop","text":"<ol> <li> <p>Open a terminal and navigate to the src/csharp/workshop/AgentWorkshop.Client folder.</p> <pre><code>cd build-your-first-agent-with-azure-ai-agent-service-workshop\\src\\csharp\\workshop\\AgentWorkshop.Client\n</code></pre> </li> <li> <p>Add the Project endpoint you copied from Azure AI Foundry to the user secrets.</p> <pre><code>dotnet user-secrets set \"ConnectionStrings:AiAgentService\" \"&lt;your_project_endpoint&gt;\"\n</code></pre> </li> <li> <p>Add the Model deployment name to the user secrets.</p> <pre><code>dotnet user-secrets set \"Azure:ModelName\" \"gpt-4o-mini\"\n</code></pre> </li> </ol>"},{"location":"includes/introduction-event/#project-structure_1","title":"Project Structure","text":"<p>Be sure to familiarize yourself with the key subfolders and files you\u2019ll be working with throughout the workshop.</p>"},{"location":"includes/introduction-event/#the-workshop-folder","title":"The workshop folder","text":"<ul> <li>The Lab1.cs, Lab2.cs, Lab3.cs files: The entry point for each lab, containing its agent logic.</li> <li>The Program.cs file: The entry point for the app, containing its main logic.</li> <li>The SalesData.cs file: The function logic to execute dynamic SQL queries against the SQLite database.</li> </ul>"},{"location":"includes/introduction-event/#the-shared-folder","title":"The shared folder","text":"<ul> <li>The files folder: Contains the files created by the agent app.</li> <li>The fonts folder: Contains the multilingual fonts used by Code Interpreter.</li> <li>The instructions folder: Contains the instructions passed to the LLM.</li> </ul>"},{"location":"includes/introduction-event/#pro-tips","title":"Pro Tips","text":"<p>Tips</p> <ol> <li>The Burger Menu in the right-hand panel of the lab environment offers additional features, including the Split Window View and the option to end the lab. The Split Window View allows you to maximize the lab environment to full screen, optimizing screen space. The lab's Instructions and Resources panel will open in a separate window.</li> <li>If the lab instructions are slow to scroll in the lab environment, try copying the instructions\u2019 URL and opening it in your computer\u2019s local browser for a smoother experience.</li> <li>If you have trouble viewing an image, simply click the image to enlarge it.</li> </ol>"},{"location":"includes/introduction-self-guided/","title":"Introduction self guided","text":""},{"location":"includes/introduction-self-guided/#self-guided-learners","title":"Self-Guided Learners","text":"<p>These instructions are for self-guided learners who do not have access to a pre-configured lab environment. Follow these steps to set up your environment and begin the workshop.</p>"},{"location":"includes/introduction-self-guided/#introduction","title":"Introduction","text":"<p>This workshop is designed to teach you about the Azure AI Agents Service and the associated SDK. It consists of multiple labs, each highlighting a specific feature of the Azure AI Agents Service. The labs are meant to be completed in order, as each one builds on the knowledge and work from the previous lab.</p>"},{"location":"includes/introduction-self-guided/#prerequisites","title":"Prerequisites","text":"<ol> <li>Access to an Azure subscription. If you don't have an Azure subscription, create a free account before you begin.</li> <li>You need a GitHub account. If you don\u2019t have one, create it at GitHub.</li> </ol>"},{"location":"includes/introduction-self-guided/#select-workshop-programming-language","title":"Select Workshop Programming Language","text":"<p>The workshop is available in both Python and C#. Use the language selector tabs to choose your preferred language. Note, don't switch languages mid-workshop.</p> <p>Select the tab for your preferred language:</p> PythonC# <p>The default language for the workshop is set to Python.</p> <p>The default language for the workshop is set to C#.</p>"},{"location":"includes/introduction-self-guided/#open-the-workshop","title":"Open the Workshop","text":"<p>The preferred way to run this workshop is using GitHub Codespaces. This option provides a pre-configured environment with all the tools and resources needed to complete the workshop. Alternatively, you can open the workshop locally using a Visual Studio Code Dev Container.</p> GitHub CodespacesVS Code Dev Container <p>Select Open in GitHub Codespaces to open the project in GitHub Codespaces.</p> <p></p> <p>Building the Codespace will take several minutes. You can continue reading the instructions while it builds.</p> <p>Apple Silicon Users</p> <p>The automated deployment script you\u2019ll be running soon isn\u2019t supported on Apple Silicon. Please run the deployment script from Codespaces or from macOS instead of the Dev Container.</p> <p>Alternatively, you can open the project locally using a Visual Studio Code Dev Container, which will open the project in your local VS Code development environment using the Dev Containers extension.</p> <ol> <li>Start Docker Desktop (install it if not already installed)</li> <li> <p>Select Dev Containers Open to open the project in a VS Code Dev Container.</p> <p></p> </li> </ol> <p>The process of building the Dev Container, which involves downloading and setting it up on your local system, will take several minutes. During this time, you can continue reading the instructions.</p>"},{"location":"includes/introduction-self-guided/#authenticate-with-azure","title":"Authenticate with Azure","text":"<p>You need to authenticate with Azure so the agent app can access the Azure AI Agents Service and models. Follow these steps:</p> <ol> <li>Ensure the Codespace has been created.</li> <li>In the Codespace, open a new terminal window by selecting Terminal &gt; New Terminal from the VS Code menu.</li> <li> <p>Run the following command to authenticate with Azure:</p> <pre><code>az login --use-device-code\n</code></pre> <p>Note</p> <p>You'll be prompted to open a browser link and log in to your Azure account. Be sure to copy the authentication code first.</p> <ol> <li>A browser window will open automatically, select your account type and click Next.</li> <li>Sign in with your Azure subscription Username and Password.</li> <li>Paste the authentication code.</li> <li>Select OK, then Done.</li> </ol> <p>Warning</p> <p>If you have multiple Azure tenants, then you will need to select the appropriate tenant when authenticating.</p> <pre><code>az login --use-device-code --tenant &lt;tenant_id&gt;\n</code></pre> </li> <li> <p>Next, select the appropriate subscription from the command line.</p> </li> <li>Leave the terminal window open for the next steps.</li> </ol>"},{"location":"includes/introduction-self-guided/#deploy-the-azure-resources","title":"Deploy the Azure Resources","text":"<p>The following resources will be created in the rg-contoso-agent-workshop-nnnn resource group in your Azure subscription.</p> <ul> <li>An Azure AI Foundry hub named fdy-contoso-agent-nnnn</li> <li>An Azure AI Foundry project named prj-contoso-agent-nnnn</li> <li>A Serverless (pay-as-you-go) GPT-4o-mini model deployment named gpt-4o-mini. See pricing details here.</li> </ul> <p>You will need 120K TPM quota availability for the gpt-4o-mini Global Standard SKU, not because the agent uses lots of tokens, but due to the frequency of calls made by the agent to the model. Review your quota availability in the AI Foundry Management Center.</p> <p>We have provided a bash script to automate the deployment of the resources required for the workshop.</p> <p>The script <code>deploy.sh</code> deploys to the <code>westus</code> region by default; edit the file to change the region or resource names. To run the script, open the VS Code terminal and run the following command:</p> <pre><code>cd infra &amp;&amp; ./deploy.sh\n</code></pre>"},{"location":"includes/introduction-self-guided/#workshop-configuration","title":"Workshop Configuration","text":"PythonC# <p>The deploy script generates the .env file, which contains the project endpoint, model deployment name. </p> <p>You'll see this file when you open the Python workspace in VS Code. Your .env file will look similar to this but with your project endpoint.</p> <pre><code>MODEL_DEPLOYMENT_NAME=\"gpt-4o-mini\"\nPROJECT_ENDPOINT=\"&lt;your_project_endpoint&gt;\"\n</code></pre> <p>The automated deployment script stores project variables securely by using the Secret Manager feature for safe storage of app secrets in development in ASP.NET Core.</p> <p>You can view the secrets by running the following command after you have opened the C# workspace in VS Code:</p> <pre><code>dotnet user-secrets list\n</code></pre>"},{"location":"includes/introduction-self-guided/#selecting-the-language-workspace","title":"Selecting the Language Workspace","text":"<p>There are two workspaces in the workshop, one for Python and one for C#. The workspace contains the source code and all the files needed to complete the labs for each language. Choose the workspace that matches the language you want to work with.</p> PythonC# <ol> <li>In Visual Studio Code, go to File &gt; Open Workspace from File.</li> <li> <p>Replace the default path with the following:</p> <pre><code>/workspaces/build-your-first-agent-with-azure-ai-agent-service-workshop/.vscode/\n</code></pre> </li> <li> <p>Choose the file named python-workspace.code-workspace to open the workspace.</p> </li> </ol> <ol> <li>In Visual Studio Code, go to File &gt; Open Workspace from File.</li> <li> <p>Replace the default path with the following:</p> <pre><code>/workspaces/build-your-first-agent-with-azure-ai-agent-service-workshop/.vscode/\n</code></pre> </li> <li> <p>Choose the file named csharp-workspace.code-workspace to open the workspace.</p> </li> </ol>"},{"location":"includes/introduction-self-guided/#project-structure","title":"Project Structure","text":"<p>Be sure to familiarize yourself with the key folders and files you\u2019ll be working with throughout the workshop.</p>"},{"location":"includes/introduction-self-guided/#the-workshop-folder","title":"The workshop folder","text":"<ul> <li>The main.py file: The entry point for the app, containing its main logic.</li> <li>The sales_data.py file: The function logic to execute dynamic SQL queries against the SQLite database.</li> <li>The stream_event_handler.py file: Contains the event handler logic for token streaming.</li> </ul>"},{"location":"includes/introduction-self-guided/#the-shared-folder","title":"The shared folder","text":"<ul> <li>The files folder: Contains the files created by the agent app.</li> <li>The fonts folder: Contains the multilingual fonts used by Code Interpreter.</li> <li>The instructions folder: Contains the instructions passed to the LLM.</li> </ul>"},{"location":"includes/introduction-self-guided/#project-structure_1","title":"Project Structure","text":"<p>Be sure to familiarize yourself with the key folders and files you\u2019ll be working with throughout the workshop.</p>"},{"location":"includes/introduction-self-guided/#the-workshop-folder_1","title":"The workshop folder","text":"<ul> <li>The Lab1.cs, Lab2.cs, Lab3.cs files: The entry point for each lab, containing its agent logic.</li> <li>The Program.cs file: The entry point for the app, containing its main logic.</li> <li>The SalesData.cs file: The function logic to execute dynamic SQL queries against the SQLite database.</li> </ul>"},{"location":"includes/introduction-self-guided/#the-shared-folder_1","title":"The shared folder","text":"<ul> <li>The files folder: Contains the files created by the agent app.</li> <li>The fonts folder: Contains the multilingual fonts used by Code Interpreter.</li> <li>The instructions folder: Contains the instructions passed to the LLM.</li> </ul>"},{"location":"includes/lab-6-finishing-up-event/","title":"Lab 6 finishing up event","text":"<p>That's all for the lab portion of this workshop. Read on for key takeaways and additional resources, but first let's make it easy for you to retrieve and re-use this workshop material back home.</p>"},{"location":"includes/lab-6-finishing-up-event/#star-the-github-repository","title":"Star the GitHub Repository","text":"<p>If you have a GitHub account, you can \"star\" this repository to make it easy for you to find again in the future.</p> <ul> <li>Visit the GitHub repository at: microsoft/build-your-first-agent-with-azure-ai-agent-service-workshop</li> <li>Log into your GitHub account</li> <li>Click Star in the top right</li> </ul> <p>To find this workshop again in the future, click your GitHub profile picture in the top-right and click Your stars.</p>"},{"location":"includes/lab-6-finishing-up-self-guided/","title":"Lab 6 finishing up self guided","text":"<p>That's all for the lab portion of this workshop. Read on for key takeaways and additional resources, but first let's tidy up.</p>"},{"location":"includes/lab-6-finishing-up-self-guided/#star-the-github-repository","title":"Star the GitHub Repository","text":"<p>If you have a GitHub account, you can \"star\" this repository to make it easy for you to find again in the future.</p> <ul> <li>Visit the GitHub repository at: microsoft/build-your-first-agent-with-azure-ai-agent-service-workshop</li> <li>Log into your GitHub account</li> <li>Click Star in the top right</li> </ul> <p>To find this workshop again in the future, click your GitHub profile picture in the top-right and click Your stars.</p>"},{"location":"includes/lab-6-finishing-up-self-guided/#clean-up-github-codespaces","title":"Clean up GitHub CodeSpaces","text":""},{"location":"includes/lab-6-finishing-up-self-guided/#save-your-changes-in-github","title":"Save your changes in GitHub","text":"<p>You can save any changes you have made to files during the workshop to your personal GitHub repository as a fork. This makes it easy to re-run the workshop with your customizations, and the workshop content will always remain available in your GitHub account.</p> <ul> <li>In VS Code, click the \"Source Control\" tool in the left pane. It's the third one down, or you can use the keyboard shortcut Control-Shift-G.</li> <li>In the field under \"Source Control\" enter <code>Agents Lab</code> and click \u2714\ufe0fCommit.</li> <li>Click Yes to the prompt \"There are no staged changes to commit.\"</li> <li>Click Sync Changes.</li> <li>Click OK to the prompt \"This action will pull and push commits from and to origin/main\".</li> </ul> <p>You now have your own copy of the workshop with your customizations in your GitHub account.</p>"},{"location":"includes/lab-6-finishing-up-self-guided/#delete-your-github-codespace","title":"Delete your GitHub codespace","text":"<p>Your GitHub CodeSpace will shut down by itself, but it will consume a small amount of your compute and storage allotment until it is deleted. (You can see your usage in your GitHub Billing summary.) You can safely delete the codespace now, as follows:</p> <ul> <li>Visit github.com/codespaces</li> <li>At the bottom of the page, click the \"...\" menu to the right of your active codespace</li> <li>Click Delete</li> <li>At the \"Are you sure?\" prompt, click Delete.</li> </ul>"},{"location":"includes/lab-6-finishing-up-self-guided/#delete-your-azure-resources","title":"Delete your Azure resources","text":"<p>Most of the resources you created in this lab are pay-as-you-go resources, meaning you won't be charged any more for using them. However, some storage services used by AI Foundry may incur small ongoing charges. To delete all resources, follow these steps:</p> <ul> <li>Visit the Azure Portal</li> <li>Click Resource groups</li> <li>Click on your resource group <code>rg-agent-workshop-****</code></li> <li>Click Delete Resource group</li> <li>In the field at the bottom \"Enter resource group name to confirm deletion\" enter <code>rg-agent-workshop-****</code></li> <li>Click Delete</li> <li>At the Delete Confirmation prompt, click \"Delete\"</li> </ul>"}]}